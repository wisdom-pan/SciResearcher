# åˆå­¦è€…æ•™ç¨‹ï¼šSciResearcher é¡¹ç›®å®Œå…¨æŒ‡å—

> ğŸ¯ **æ•™ç¨‹ç›®æ ‡**ï¼šå¸®åŠ©é›¶åŸºç¡€çš„å‚èµ›è€…ä»å…¥é—¨åˆ°å®Œæˆå‚èµ›é¡¹ç›®çš„å…¨è¿‡ç¨‹æŒ‡å¯¼

---

## ğŸ“š ç›®å½•

1. [æ¯”èµ›ä»‹ç»](#1-æ¯”èµ›ä»‹ç»)
2. [é¡¹ç›®èƒŒæ™¯ä¸å®šä½](#2-é¡¹ç›®èƒŒæ™¯ä¸å®šä½)
3. [æ ¸å¿ƒæ¦‚å¿µè§£æ](#3-æ ¸å¿ƒæ¦‚å¿µè§£æ)
4. [æŠ€æœ¯æ¶æ„è¯¦è§£](#4-æŠ€æœ¯æ¶æ„è¯¦è§£)
5. [ç¯å¢ƒå‡†å¤‡](#5-ç¯å¢ƒå‡†å¤‡)
6. [åˆ†æ­¥å®ç°æŒ‡å—](#6-åˆ†æ­¥å®ç°æŒ‡å—)
7. [æäº¤è¦æ±‚](#7-æäº¤è¦æ±‚)
8. [å¸¸è§é—®é¢˜è§£ç­”](#8-å¸¸è§é—®é¢˜è§£ç­”)
9. [å­¦ä¹ èµ„æº](#9-å­¦ä¹ èµ„æº)

---

## 1. æ¯”èµ›ä»‹ç»

### 1.1 èµ›é¢˜ä¿¡æ¯

**èµ›é¢˜åç§°**ï¼šAI+ç§‘ç ”åˆ›æ–°èµ›é“

**èµ›é¢˜ä¸»é¢˜**ï¼š"æ„å»ºå¼€æºç§‘ç ”ç”Ÿæ€,åŠ é€Ÿç§‘å­¦å‘ç°"

**å‚èµ›ç±»åˆ«**ï¼šç§‘ç ”æ¡†æ¶ï¼ˆScientific Frameworkï¼‰

### 1.2 èµ›é¢˜è¦æ±‚å¯¹ç…§è¡¨

| è¦æ±‚ç±»åˆ« | å…·ä½“è¦æ±‚ | SciResearcher å¯¹åº”å†…å®¹ |
|---------|---------|---------------------|
| **ç§‘ç ”æ¨¡å‹** | ç‰¹å®šç§‘ç ”é¢†åŸŸçš„åˆ›æ–°AIæ¨¡å‹ | âœ… Qwen3 ç³»åˆ—æ¨¡å‹ + å¾®è°ƒèƒ½åŠ› |
| **ç§‘ç ”åº”ç”¨** | åŸºäºAIçš„ç§‘ç ”å·¥å…·æˆ–å¹³å° | âœ… æ–‡çŒ®æ·±åº¦ç†è§£ + å¤šæ¨¡æ€åˆ†æ |
| **ç§‘ç ”æ¡†æ¶** | æ”¯æŒç§‘ç ”å·¥ä½œæµçš„å¼€æºæ¡†æ¶ | âœ… Multi-Agent åä½œæ¡†æ¶ |
| **å¼€æºå‘å¸ƒ** | åœ¨é­”æ­ç¤¾åŒºå‘å¸ƒå®Œæ•´é¡¹ç›® | âœ… éœ€è¦å‡†å¤‡ |
| **å¯å¤ç°æ€§** | æä¾›å®éªŒç»“æœå’Œè¯„ä¼°åŸºå‡† | âœ… éœ€è¦å‡†å¤‡ |
| **åº”ç”¨éªŒè¯** | çœŸå®ç§‘ç ”åœºæ™¯éªŒè¯ | âœ… PDF æ–‡çŒ®ç†è§£åœºæ™¯ |

### 1.3 æäº¤ç‰©æ¸…å•

```
ğŸ“¦ æäº¤ææ–™æ£€æŸ¥æ¸…å•
â”œâ”€â”€ âœ… å¼€æºé¡¹ç›®ä»£ç åŒ…ï¼ˆå«æ¨¡å‹ã€å·¥å…·ã€æ¡†æ¶ï¼‰
â”œâ”€â”€ âœ… å®Œæ•´æŠ€æœ¯æ–‡æ¡£ï¼ˆå®‰è£…æŒ‡å—ã€APIè¯´æ˜ã€ä½¿ç”¨ç¤ºä¾‹ï¼‰
â”œâ”€â”€ âœ… å¯å¤ç°çš„å®éªŒæŠ¥å‘Šï¼ˆæ•°æ®é›†ã€è®­ç»ƒæµç¨‹ã€è¯„ä¼°æŒ‡æ ‡ï¼‰
â”œâ”€â”€ âœ… åº”ç”¨éªŒè¯æ¡ˆä¾‹ï¼ˆçœŸå®ç§‘ç ”ä»»åŠ¡æ¼”ç¤ºï¼‰
â””â”€â”€ âœ… è§†é¢‘æˆ–å›¾æ–‡è¯´æ˜ï¼ˆåº”ç”¨ä»·å€¼å±•ç¤ºï¼‰
```

---

## 2. é¡¹ç›®èƒŒæ™¯ä¸å®šä½

### 2.1 ç§‘ç ”å·¥ä½œè€…çš„ç—›ç‚¹

#### ğŸ“Š ç—›ç‚¹åˆ†æå›¾

```
ä¿¡æ¯è¿‡è½½                ç†è§£æ·±åº¦ä½              AIå¹»è§‰é«˜               å¤šæ¨¡æ€å¤„ç†ç‰‡é¢
    â†“                      â†“                      â†“                        â†“
arXivæ¯æ—¥200+è®ºæ–‡      åªèƒ½æ‘˜è¦/å…³é”®è¯        æ— å¼•ç”¨ã€è™šæ„ç»“è®º         å›¾è¡¨å…¬å¼æœªè¢«åˆ©ç”¨
    â†“                      â†“                      â†“                        â†“
äººå·¥é˜…è¯»æ•ˆç‡ä½         æ— æ³•å›ç­”é«˜é˜¶é—®é¢˜        ç§‘ç ”è¯¯åˆ¤é£é™©           60%+ç»“è®ºä¾èµ–å›¾è¡¨
```

#### ğŸ” ç°æœ‰å·¥å…·çš„é—®é¢˜

| å·¥å…·ç±»å‹ | ä»£è¡¨äº§å“ | ä¸»è¦é—®é¢˜ |
|---------|---------|---------|
| **å•ç‚¹å·¥å…·** | MinerUã€LlamaIndex | åŠŸèƒ½å•ä¸€,æ— æ³•åä½œ |
| **é€šç”¨Agentæ¡†æ¶** | AutoGenã€LangGraph | ç¼ºä¹ç§‘ç ”ç‰¹åŒ–è®¾è®¡ |
| **å°é—­å•†ä¸šç³»ç»Ÿ** | Sciteã€Consensus | å®šåˆ¶åŒ–å›°éš¾,éš¾æ‰©å±• |

### 2.2 é¡¹ç›®å®šä½

```
ğŸ¯ SciResearcher = é¦–ä¸ªåŸºäº smolagents + Qwen3 + MinerU2.5 çš„ç§‘ç ”æ–‡çŒ®ç ”ç©¶æ¡†æ¶

æ ¸å¿ƒç‰¹ç‚¹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ¨ è½»é‡çº§ï¼šä¸ä¾èµ–é‡å‹æ¡†æ¶              â”‚
â”‚ ğŸ”“ å¼€æºï¼šå®Œå…¨å¼€æºå¯å®šåˆ¶                â”‚
â”‚ âœ… å¯éªŒè¯ï¼šå¼ºåˆ¶å¼•ç”¨ + ç½®ä¿¡åº¦ + è‡ªæˆ‘æ ¡éªŒ â”‚
â”‚ ğŸ¤– Agentåä½œï¼š5ä¸ªæ™ºèƒ½ä½“åˆ†å·¥åä½œ        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 MVPç›®æ ‡ä¸æ‹“å±•æ€è·¯

#### MVPç›®æ ‡ï¼ˆå¿…é¡»å®Œæˆï¼‰

```mermaid
graph LR
    A[Planning Agent] --> B[4ä¸ªSub-Agentåä½œ]
    B --> C[PDFå¤šæ¨¡æ€è§£æ]
    C --> D[å›¾åƒç†è§£]
    D --> E[å¸¦å¼•ç”¨+ç½®ä¿¡åº¦ç­”æ¡ˆ]
    E --> F[MCPé›†æˆ]
```

#### æ‹“å±•æ€è·¯ï¼ˆå¯é€‰åŠ åˆ†é¡¹ï¼‰

- ğŸ”¹ å¤šæ–‡æ¡£ç»¼è¿°
- ğŸ”¹ å®éªŒè®¾è®¡å»ºè®®
- ğŸ”¹ ç ”ç©¶ç©ºç™½æ¢æµ‹
- ğŸ”¹ Hugging Face daily paperæ”¯æŒ
- ğŸ”¹ æ¯ç§agent toolå°è£…ä¸ºMCP

---

## 3. æ ¸å¿ƒæ¦‚å¿µè§£æ

### 3.1 ä»€ä¹ˆæ˜¯Multi-Agent?

**ç®€å•ç†è§£**ï¼šå°±åƒä¸€ä¸ªç ”ç©¶å›¢é˜Ÿ,æ¯ä¸ªäººè´Ÿè´£ä¸åŒçš„ä»»åŠ¡

```
ä¼ ç»Ÿæ–¹å¼ï¼šä¸€ä¸ªAIåšæ‰€æœ‰äº‹æƒ…
    AI â†’ è¯»è®ºæ–‡ â†’ ç†è§£å›¾è¡¨ â†’ å›ç­”é—®é¢˜ â†’ æ£€æŸ¥ç­”æ¡ˆ
    é—®é¢˜ï¼šæ ·æ ·é€š,æ ·æ ·æ¾

Multi-Agentæ–¹å¼ï¼šå¤šä¸ªAIå„å¸å…¶èŒ
    Planner â†’ æ‹†è§£ä»»åŠ¡
    Retriever â†’ æ£€ç´¢å†…å®¹
    Caption Agent â†’ ç†è§£å›¾åƒ
    Reasoner â†’ æ¨ç†ç”Ÿæˆ
    Reviewer â†’ è´¨é‡æ£€æŸ¥
    ä¼˜åŠ¿ï¼šä¸“ä¸šåˆ†å·¥,è´¨é‡æ›´é«˜
```

### 3.2 ä»€ä¹ˆæ˜¯smolagents?

**å®˜æ–¹å®šä¹‰**ï¼šHugging Faceå¼€å‘çš„è½»é‡çº§Agentæ¡†æ¶

**ä¸ºä»€ä¹ˆé€‰å®ƒ**ï¼š
- âœ… ç®€å•æ˜“ç”¨,ä»£ç é‡å°‘
- âœ… ä¸Hugging Faceç”Ÿæ€é›†æˆå¥½
- âœ… æ”¯æŒQwen3ç­‰å¼€æºæ¨¡å‹
- âœ… æ–‡æ¡£é½å…¨,ç¤¾åŒºæ´»è·ƒ

**åŸºç¡€ç¤ºä¾‹**ï¼š

```python
from smolagents import CodeAgent, HfApiModel

# åˆ›å»ºä¸€ä¸ªAgent
model = HfApiModel("Qwen/Qwen2.5-Coder-32B-Instruct")
agent = CodeAgent(tools=[], model=model)

# è¿è¡ŒAgent
agent.run("åˆ†æè¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®")
```

### 3.3 ä»€ä¹ˆæ˜¯MinerU?

**å®˜æ–¹å®šä¹‰**ï¼šPDFè§£æå·¥å…·,æ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€è¡¨æ ¼ã€å…¬å¼æå–

**ä¸ºä»€ä¹ˆéœ€è¦å®ƒ**ï¼š
- ğŸ“„ ç§‘ç ”è®ºæ–‡æ˜¯PDFæ ¼å¼
- ğŸ“Š åŒ…å«å¤§é‡å›¾è¡¨ã€å…¬å¼
- ğŸ” æ™®é€šPDFè§£æå™¨æ— æ³•å‡†ç¡®æå–

**è§£ææ•ˆæœå¯¹æ¯”**ï¼š

| è§£æå™¨ | æ–‡å­— | å›¾ç‰‡ | è¡¨æ ¼ | å…¬å¼ |
|-------|------|------|------|------|
| PyPDF2 | âœ… | âŒ | âŒ | âŒ |
| pdfplumber | âœ… | âš ï¸ | âš ï¸ | âŒ |
| **MinerU** | âœ… | âœ… | âœ… | âœ… |

### 3.4 ä»€ä¹ˆæ˜¯RAG?

**å…¨ç§°**ï¼šRetrieval-Augmented Generationï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰

**é€šä¿—è§£é‡Š**ï¼š

```
æ²¡æœ‰RAGï¼š
    ç”¨æˆ·é—®é¢˜ â†’ AIå‡­è®°å¿†å›ç­” â†’ å®¹æ˜“çç¼–ï¼ˆå¹»è§‰ï¼‰

æœ‰RAGï¼š
    ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³å†…å®¹ â†’ AIåŸºäºæ£€ç´¢ç»“æœå›ç­” â†’ æœ‰ä¾æ®,æ›´å‡†ç¡®
```

**SciResearcherçš„RAGæµç¨‹**ï¼š

```
1. PDFè§£æï¼ˆMinerUï¼‰â†’ æå–æ–‡å­—ã€å›¾ç‰‡ã€å…¬å¼
2. å‘é‡åŒ–ï¼ˆQwen3-Embeddingï¼‰â†’ è½¬æ¢ä¸ºæ•°å­—å‘é‡
3. å­˜å‚¨ï¼ˆVector DBï¼‰â†’ ä¿å­˜åˆ°æ•°æ®åº“
4. æ£€ç´¢ï¼ˆRetriever Agentï¼‰â†’ æ ¹æ®é—®é¢˜æ‰¾ç›¸å…³å†…å®¹
5. ç”Ÿæˆï¼ˆReasoner Agentï¼‰â†’ åŸºäºæ£€ç´¢ç»“æœå›ç­”
```

---

## 4. æŠ€æœ¯æ¶æ„è¯¦è§£

### 4.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ·è¾“å…¥é—®é¢˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Planner Agent                             â”‚
â”‚  èŒè´£ï¼šä»»åŠ¡åˆ†è§£                                              â”‚
â”‚  è¾“å…¥ï¼šç”¨æˆ·é—®é¢˜                                              â”‚
â”‚  è¾“å‡ºï¼šå­ä»»åŠ¡åˆ—è¡¨ [sub_task_1, sub_task_2, ...]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“              â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retriever  â”‚ â”‚Caption Agentâ”‚ â”‚  Reasoner   â”‚
â”‚   Agent     â”‚ â”‚             â”‚ â”‚   Agent     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚å¤šæ¨¡æ€æ£€ç´¢    â”‚ â”‚å›¾åƒç†è§£      â”‚ â”‚æ¨ç†ç”Ÿæˆ      â”‚
â”‚Qwen3-       â”‚ â”‚Qwen3-VL     â”‚ â”‚VLç³»åˆ—æ¨¡å‹    â”‚
â”‚Embedding    â”‚ â”‚+MinerU      â”‚ â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Reviewer Agent                            â”‚
â”‚  èŒè´£ï¼šè‡ªæˆ‘æ ¡éªŒ                                              â”‚
â”‚  è¾“å…¥ï¼šç­”æ¡ˆ + è¯æ®                                           â”‚
â”‚  è¾“å‡ºï¼šæœ€ç»ˆç­”æ¡ˆ + ç½®ä¿¡åº¦ + æ˜¯å¦éœ€è¦è¿­ä»£                      â”‚
â”‚  æŠ€æœ¯ï¼šRule based + LLM Judge                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æœ€ç»ˆè¾“å‡ºï¼ˆå¸¦å¼•ç”¨+ç½®ä¿¡åº¦ï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Agentè¯¦ç»†è®¾è®¡è¡¨

| Agentåç§° | èŒè´£ | è¾“å…¥ | è¾“å‡º | æŠ€æœ¯å®ç° |
|----------|------|------|------|---------|
| **Planner** | ä»»åŠ¡åˆ†è§£ | `{question: str}` | `{sub_tasks: List[str]}` | LLM Prompt based |
| **Retriever** | å¤šæ¨¡æ€æ£€ç´¢ | `{sub_tasks: List[str]}` | `{evidence: List[Evidence]}` | Qwen3-Embedding + Vector DB |
| **Caption Agent** | å›¾åƒç†è§£ | `{image_path: str, task: str}` | `{description: str}` | Qwen3-VL + MinerU |
| **Reasoner** | æ¨ç†ç”Ÿæˆ | `{question: str, evidence: List}` | `{answer: str, confidence: float}` | VLç³»åˆ—æ¨¡å‹ |
| **Reviewer** | è‡ªæˆ‘æ ¡éªŒ | `{answer: str, evidence: List}` | `{final_answer: str, confidence: float, need_iterate: bool}` | Rule based + LLM Judge |

### 4.3 æ•°æ®æµè½¬å›¾

```
PDFæ–‡æ¡£
   â†“ [MinerUè§£æ]
æ–‡å­— + å›¾ç‰‡ + è¡¨æ ¼ + å…¬å¼
   â†“ [Qwen3-Embeddingå‘é‡åŒ–]
å‘é‡æ•°æ®åº“
   â†“ [ç”¨æˆ·æé—®]
Planneræ‹†è§£ä»»åŠ¡
   â†“ [ä»»åŠ¡åˆ†é…]
â”œâ”€ Retrieveræ£€ç´¢æ–‡å­—è¯æ®
â”œâ”€ Caption Agentç†è§£å›¾è¡¨
â””â”€ æ±‡æ€»è¯æ®
   â†“ [æ¨ç†]
Reasonerç”Ÿæˆç­”æ¡ˆ
   â†“ [è´¨æ£€]
Revieweræ ¡éªŒ
   â”œâ”€ é€šè¿‡ â†’ è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
   â””â”€ ä¸é€šè¿‡ â†’ è¿”å›Retrieveré‡æ–°æ£€ç´¢
```

### 4.4 æŠ€æœ¯æ ˆæ¸…å•

#### æ ¸å¿ƒæ¡†æ¶

```yaml
Agentæ¡†æ¶:
  - smolagents: è½»é‡çº§Multi-Agentæ¡†æ¶

æ¨¡å‹:
  - Qwen3-Embedding: æ–‡æœ¬å‘é‡åŒ–
  - Qwen3-VL: å›¾åƒç†è§£
  - Qwen2.5-Coder-32B-Instruct: æ¨ç†ç”Ÿæˆ

PDFè§£æ:
  - MinerU 2.5: å¤šæ¨¡æ€PDFè§£æ

å‘é‡æ•°æ®åº“:
  - FAISS / ChromaDB / Milvus (ä¸‰é€‰ä¸€)
```

#### å¼€å‘ç¯å¢ƒ

```yaml
ç¼–ç¨‹è¯­è¨€:
  - Python 3.10+

éƒ¨ç½²æ–¹å¼:
  - vLLM: é«˜æ€§èƒ½æ¨ç†
  - SGLang: ç»“æ„åŒ–ç”Ÿæˆ

å¾®è°ƒå·¥å…·:
  - SFT: ç›‘ç£å¾®è°ƒ
  - LoRA: å‚æ•°é«˜æ•ˆå¾®è°ƒ
  - GRPO: å¼ºåŒ–å­¦ä¹ å¾®è°ƒ
```

---

## 5. ç¯å¢ƒå‡†å¤‡

### 5.1 ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|---------|---------|
| **CPU** | 8æ ¸ | 16æ ¸+ |
| **å†…å­˜** | 32GB | 64GB+ |
| **GPU** | NVIDIA RTX 3090 (24GB) | A100 (40GB/80GB) |
| **å­˜å‚¨** | 100GB | 500GB+ SSD |

### 5.2 è½¯ä»¶å®‰è£…æ­¥éª¤

#### æ­¥éª¤1ï¼šå®‰è£…Pythonç¯å¢ƒ

```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version  # éœ€è¦ >= 3.10

# å¦‚æœç‰ˆæœ¬ä¸ç¬¦,ä½¿ç”¨condaåˆ›å»ºæ–°ç¯å¢ƒ
conda create -n sciresearcher python=3.10
conda activate sciresearcher
```

#### æ­¥éª¤2ï¼šå®‰è£…PyTorch

```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯GPU
python -c "import torch; print(torch.cuda.is_available())"
```

#### æ­¥éª¤3ï¼šå®‰è£…æ ¸å¿ƒä¾èµ–

```bash
# smolagents
pip install smolagents

# MinerU
pip install "magic-pdf[full]==0.7.0b1" --extra-index-url https://wheels.myhloli.com

# Qwenæ¨¡å‹
pip install transformers>=4.37.0
pip install accelerate
pip install sentencepiece

# å‘é‡æ•°æ®åº“ï¼ˆé€‰ä¸€ä¸ªï¼‰
pip install faiss-cpu  # CPUç‰ˆæœ¬
pip install faiss-gpu  # GPUç‰ˆæœ¬
# æˆ–è€…
pip install chromadb
```

#### æ­¥éª¤4ï¼šä¸‹è½½æ¨¡å‹

```python
from huggingface_hub import snapshot_download

# ä¸‹è½½Qwen3-Embedding
snapshot_download(
    repo_id="Qwen/Qwen2.5-Embed-1.5B",
    local_dir="./models/qwen-embed"
)

# ä¸‹è½½Qwen3-VL
snapshot_download(
    repo_id="Qwen/Qwen2-VL-7B-Instruct",
    local_dir="./models/qwen-vl"
)

# ä¸‹è½½Qwen2.5-Coder
snapshot_download(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    local_dir="./models/qwen-coder"
)
```

### 5.3 é¡¹ç›®ç»“æ„

```
SciResearcher/
â”œâ”€â”€ agents/                 # Agentå®ç°
â”‚   â”œâ”€â”€ planner.py         # Planner Agent
â”‚   â”œâ”€â”€ retriever.py       # Retriever Agent
â”‚   â”œâ”€â”€ caption.py         # Caption Agent
â”‚   â”œâ”€â”€ reasoner.py        # Reasoner Agent
â”‚   â””â”€â”€ reviewer.py        # Reviewer Agent
â”œâ”€â”€ models/                 # æ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ qwen-embed/
â”‚   â”œâ”€â”€ qwen-vl/
â”‚   â””â”€â”€ qwen-coder/
â”œâ”€â”€ tools/                  # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ pdf_parser.py      # PDFè§£æ
â”‚   â””â”€â”€ vector_db.py       # å‘é‡æ•°æ®åº“
â”œâ”€â”€ data/                   # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ pdfs/              # åŸå§‹PDF
â”‚   â””â”€â”€ processed/         # å¤„ç†åæ•°æ®
â”œâ”€â”€ experiments/            # å®éªŒç»“æœ
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ config.yaml            # é…ç½®æ–‡ä»¶
â””â”€â”€ main.py                # ä¸»ç¨‹åºå…¥å£
```

---

## 6. åˆ†æ­¥å®ç°æŒ‡å—

### 6.1 ç¬¬ä¸€æ­¥ï¼šPDFè§£ææ¨¡å—

#### ä»£ç å®ç°

```python
# tools/pdf_parser.py
from magic_pdf.pipe.UNIPipe import UNIPipe
from magic_pdf.rw.DiskReaderWriter import DiskReaderWriter
import json
import os

class PDFParser:
    """PDFè§£æå™¨,ä½¿ç”¨MinerUæå–æ–‡å­—ã€å›¾ç‰‡ã€å…¬å¼"""

    def __init__(self, output_dir="./data/processed"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def parse(self, pdf_path):
        """
        è§£æPDFæ–‡ä»¶

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            dict: {
                "text": "æå–çš„æ–‡å­—",
                "images": ["å›¾ç‰‡1è·¯å¾„", "å›¾ç‰‡2è·¯å¾„"],
                "tables": ["è¡¨æ ¼1", "è¡¨æ ¼2"],
                "formulas": ["å…¬å¼1", "å…¬å¼2"]
            }
        """
        # åˆå§‹åŒ–MinerU
        pdf_bytes = open(pdf_path, "rb").read()

        # åˆ›å»ºè§£æå™¨
        pipe = UNIPipe(pdf_bytes, {"_pdf_type": ""}, "auto")

        # æ‰§è¡Œè§£æ
        pipe.pipe_classify()
        pipe.pipe_analyze()
        pipe.pipe_parse()

        # è·å–ç»“æœ
        content_list = pipe.pipe_mk_uni_format(pdf_path, self.output_dir)

        # æ•´ç†ç»“æœ
        result = {
            "text": "",
            "images": [],
            "tables": [],
            "formulas": []
        }

        for content in content_list:
            if content["type"] == "text":
                result["text"] += content["text"] + "\n"
            elif content["type"] == "image":
                result["images"].append(content["path"])
            elif content["type"] == "table":
                result["tables"].append(content["html"])
            elif content["type"] == "formula":
                result["formulas"].append(content["latex"])

        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    parser = PDFParser()
    result = parser.parse("./data/pdfs/sample_paper.pdf")

    print(f"æå–æ–‡å­—é•¿åº¦: {len(result['text'])}")
    print(f"å›¾ç‰‡æ•°é‡: {len(result['images'])}")
    print(f"è¡¨æ ¼æ•°é‡: {len(result['tables'])}")
    print(f"å…¬å¼æ•°é‡: {len(result['formulas'])}")
```

#### æµ‹è¯•

```bash
# ä¸‹è½½æµ‹è¯•PDF
wget https://arxiv.org/pdf/2301.00001.pdf -O ./data/pdfs/test.pdf

# è¿è¡Œæµ‹è¯•
python tools/pdf_parser.py
```

**é¢„æœŸè¾“å‡º**ï¼š

```
æå–æ–‡å­—é•¿åº¦: 15234
å›¾ç‰‡æ•°é‡: 8
è¡¨æ ¼æ•°é‡: 3
å…¬å¼æ•°é‡: 12
```

---

### 6.2 ç¬¬äºŒæ­¥ï¼šå‘é‡æ•°æ®åº“æ¨¡å—

#### ä»£ç å®ç°

```python
# tools/vector_db.py
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import numpy as np
import pickle

class VectorDatabase:
    """å‘é‡æ•°æ®åº“,ä½¿ç”¨Qwen3-Embeddingå’ŒFAISS"""

    def __init__(self, model_path="./models/qwen-embed", index_path="./data/index.faiss"):
        # åŠ è½½Embeddingæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
        self.model.eval()

        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.dimension = 1536  # Qwen3-Embeddingç»´åº¦
        self.index = faiss.IndexFlatL2(self.dimension)
        self.index_path = index_path
        self.texts = []  # å­˜å‚¨åŸå§‹æ–‡æœ¬

        # å¦‚æœå­˜åœ¨å·²æœ‰ç´¢å¼•,åŠ è½½
        if os.path.exists(index_path):
            self.load()

    def embed(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        return embedding

    def add(self, texts):
        """æ·»åŠ æ–‡æœ¬åˆ°æ•°æ®åº“"""
        embeddings = []
        for text in texts:
            embedding = self.embed(text)
            embeddings.append(embedding)

        embeddings = np.array(embeddings).astype('float32')
        self.index.add(embeddings)
        self.texts.extend(texts)

    def search(self, query, top_k=5):
        """æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æœ¬"""
        query_embedding = self.embed(query).reshape(1, -1).astype('float32')
        distances, indices = self.index.search(query_embedding, top_k)

        results = []
        for i, idx in enumerate(indices[0]):
            results.append({
                "text": self.texts[idx],
                "score": float(distances[0][i])
            })
        return results

    def save(self):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, self.index_path)
        with open(self.index_path + ".texts", "wb") as f:
            pickle.dump(self.texts, f)

    def load(self):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(self.index_path)
        with open(self.index_path + ".texts", "rb") as f:
            self.texts = pickle.load(f)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db = VectorDatabase()

    # æ·»åŠ æ–‡æœ¬
    texts = [
        "Transformeræ¨¡å‹æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„",
        "BERTä½¿ç”¨åŒå‘Transformerç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒ",
        "GPTé‡‡ç”¨è‡ªå›å½’è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ"
    ]
    db.add(texts)

    # æ£€ç´¢
    results = db.search("ä»€ä¹ˆæ˜¯Transformer?", top_k=2)
    for i, result in enumerate(results):
        print(f"\nç»“æœ{i+1}:")
        print(f"æ–‡æœ¬: {result['text']}")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")

    # ä¿å­˜
    db.save()
```

---

### 6.3 ç¬¬ä¸‰æ­¥ï¼šå®ç°5ä¸ªAgent

#### Planner Agent

```python
# agents/planner.py
from smolagents import CodeAgent, HfApiModel

class PlannerAgent:
    """ä»»åŠ¡è§„åˆ’Agent,è´Ÿè´£å°†ç”¨æˆ·é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡"""

    def __init__(self, model_path="Qwen/Qwen2.5-Coder-32B-Instruct"):
        self.model = HfApiModel(model_path)
        self.agent = CodeAgent(tools=[], model=self.model)

    def plan(self, question):
        """
        åˆ†è§£ä»»åŠ¡

        Args:
            question: ç”¨æˆ·é—®é¢˜

        Returns:
            List[str]: å­ä»»åŠ¡åˆ—è¡¨
        """
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹çš„ä»»åŠ¡è§„åˆ’å™¨ã€‚è¯·å°†ç”¨æˆ·çš„é—®é¢˜åˆ†è§£ä¸ºå…·ä½“çš„å­ä»»åŠ¡ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¯·åˆ†è§£ä¸ºä»¥ä¸‹ç±»å‹çš„å­ä»»åŠ¡:
1. æ£€ç´¢ä»»åŠ¡: éœ€è¦ä»æ–‡çŒ®ä¸­æ£€ç´¢ä»€ä¹ˆä¿¡æ¯?
2. å›¾åƒç†è§£ä»»åŠ¡: éœ€è¦åˆ†æå“ªäº›å›¾è¡¨?
3. æ¨ç†ä»»åŠ¡: éœ€è¦è¿›è¡Œä»€ä¹ˆæ ·çš„æ¨ç†?

è¯·ä»¥JSONæ ¼å¼è¿”å›å­ä»»åŠ¡åˆ—è¡¨:
{{
    "retrieval_tasks": ["ä»»åŠ¡1", "ä»»åŠ¡2"],
    "image_tasks": ["ä»»åŠ¡1", "ä»»åŠ¡2"],
    "reasoning_task": "ç»¼åˆæ¨ç†ä»»åŠ¡æè¿°"
}}
"""

        response = self.agent.run(prompt)
        # è§£æJSONå“åº”
        import json
        tasks = json.loads(response)
        return tasks

# æµ‹è¯•
if __name__ == "__main__":
    planner = PlannerAgent()
    question = "è¿™ç¯‡è®ºæ–‡æå‡ºçš„Transformeræ¨¡å‹æ¯”LSTMæœ‰ä»€ä¹ˆä¼˜åŠ¿?"
    tasks = planner.plan(question)
    print(json.dumps(tasks, indent=2, ensure_ascii=False))
```

#### Retriever Agent

```python
# agents/retriever.py
from tools.vector_db import VectorDatabase

class RetrieverAgent:
    """æ£€ç´¢Agent,è´Ÿè´£ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹"""

    def __init__(self, db_path="./data/index.faiss"):
        self.db = VectorDatabase(index_path=db_path)

    def retrieve(self, sub_tasks, top_k=5):
        """
        æ£€ç´¢è¯æ®

        Args:
            sub_tasks: å­ä»»åŠ¡åˆ—è¡¨
            top_k: æ¯ä¸ªä»»åŠ¡è¿”å›çš„ç»“æœæ•°

        Returns:
            List[dict]: æ£€ç´¢åˆ°çš„è¯æ®åˆ—è¡¨
        """
        evidence = []

        for task in sub_tasks:
            results = self.db.search(task, top_k=top_k)
            evidence.extend(results)

        # å»é‡
        seen = set()
        unique_evidence = []
        for e in evidence:
            if e["text"] not in seen:
                seen.add(e["text"])
                unique_evidence.append(e)

        return unique_evidence

# æµ‹è¯•
if __name__ == "__main__":
    retriever = RetrieverAgent()
    sub_tasks = ["Transformerçš„æ³¨æ„åŠ›æœºåˆ¶", "LSTMçš„å±€é™æ€§"]
    evidence = retriever.retrieve(sub_tasks)

    for i, e in enumerate(evidence):
        print(f"\nè¯æ®{i+1}:")
        print(f"å†…å®¹: {e['text'][:100]}...")
        print(f"ç›¸ä¼¼åº¦: {e['score']:.4f}")
```

#### Caption Agent

```python
# agents/caption.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

class CaptionAgent:
    """å›¾åƒç†è§£Agent,ä½¿ç”¨Qwen-VLåˆ†æå›¾è¡¨"""

    def __init__(self, model_path="./models/qwen-vl"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True
        ).eval()

    def caption(self, image_path, task):
        """
        ç†è§£å›¾åƒ

        Args:
            image_path: å›¾åƒè·¯å¾„
            task: ç†è§£ä»»åŠ¡æè¿°

        Returns:
            str: å›¾åƒæè¿°
        """
        query = self.tokenizer.from_list_format([
            {'image': image_path},
            {'text': f'è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾è¡¨,é‡ç‚¹å…³æ³¨: {task}'},
        ])

        response, _ = self.model.chat(
            self.tokenizer,
            query=query,
            history=None
        )

        return response

# æµ‹è¯•
if __name__ == "__main__":
    caption = CaptionAgent()
    description = caption.caption(
        "./data/processed/figure_1.png",
        "æ¨¡å‹æ¶æ„çš„ä¸»è¦ç»„ä»¶"
    )
    print(f"å›¾è¡¨æè¿°: {description}")
```

#### Reasoner Agent

```python
# agents/reasoner.py
from smolagents import CodeAgent, HfApiModel

class ReasonerAgent:
    """æ¨ç†Agent,åŸºäºè¯æ®ç”Ÿæˆç­”æ¡ˆ"""

    def __init__(self, model_path="Qwen/Qwen2.5-Coder-32B-Instruct"):
        self.model = HfApiModel(model_path)
        self.agent = CodeAgent(tools=[], model=self.model)

    def reason(self, question, evidence):
        """
        ç”Ÿæˆç­”æ¡ˆ

        Args:
            question: ç”¨æˆ·é—®é¢˜
            evidence: è¯æ®åˆ—è¡¨

        Returns:
            dict: {
                "answer": "ç­”æ¡ˆ",
                "confidence": 0.85,
                "citations": [è¯æ®ç´¢å¼•]
            }
        """
        # æ„å»ºæç¤º
        evidence_text = "\n\n".join([
            f"[è¯æ®{i+1}] {e['text']}"
            for i, e in enumerate(evidence)
        ])

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„è¯æ®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¯æ®:
{evidence_text}

è¦æ±‚:
1. ç­”æ¡ˆå¿…é¡»åŸºäºè¯æ®,ä¸èƒ½è™šæ„
2. æ˜ç¡®å¼•ç”¨ä½¿ç”¨çš„è¯æ®(å¦‚[è¯æ®1])
3. ç»™å‡ºç½®ä¿¡åº¦è¯„åˆ†(0-1)
4. å¦‚æœè¯æ®ä¸è¶³,æ˜ç¡®è¯´æ˜

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
    "answer": "ä½ çš„ç­”æ¡ˆ",
    "confidence": 0.85,
    "citations": [1, 3],
    "reasoning": "æ¨ç†è¿‡ç¨‹"
}}
"""

        response = self.agent.run(prompt)
        import json
        result = json.loads(response)
        return result

# æµ‹è¯•
if __name__ == "__main__":
    reasoner = ReasonerAgent()
    question = "Transformerç›¸æ¯”LSTMæœ‰ä»€ä¹ˆä¼˜åŠ¿?"
    evidence = [
        {"text": "Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶,å¯ä»¥å¹¶è¡Œè®¡ç®—"},
        {"text": "LSTMæ˜¯ä¸²è¡Œçš„,è®­ç»ƒé€Ÿåº¦æ…¢"}
    ]

    result = reasoner.reason(question, evidence)
    print(json.dumps(result, indent=2, ensure_ascii=False))
```

#### Reviewer Agent

```python
# agents/reviewer.py

class ReviewerAgent:
    """å®¡æ ¸Agent,æ ¡éªŒç­”æ¡ˆè´¨é‡"""

    def __init__(self, confidence_threshold=0.7):
        self.threshold = confidence_threshold

    def review(self, answer, evidence):
        """
        å®¡æ ¸ç­”æ¡ˆ

        Args:
            answer: Reasonerç”Ÿæˆçš„ç­”æ¡ˆ
            evidence: åŸå§‹è¯æ®

        Returns:
            dict: {
                "final_answer": "æœ€ç»ˆç­”æ¡ˆ",
                "confidence": 0.85,
                "need_iterate": False,
                "issues": []
            }
        """
        issues = []

        # è§„åˆ™1: æ£€æŸ¥ç½®ä¿¡åº¦
        if answer["confidence"] < self.threshold:
            issues.append(f"ç½®ä¿¡åº¦è¿‡ä½: {answer['confidence']}")

        # è§„åˆ™2: æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨
        if not answer.get("citations") or len(answer["citations"]) == 0:
            issues.append("ç¼ºå°‘å¼•ç”¨")

        # è§„åˆ™3: æ£€æŸ¥å¼•ç”¨çš„è¯æ®æ˜¯å¦å­˜åœ¨
        for cite_idx in answer.get("citations", []):
            if cite_idx > len(evidence):
                issues.append(f"å¼•ç”¨çš„è¯æ®{cite_idx}ä¸å­˜åœ¨")

        # è§„åˆ™4: æ£€æŸ¥ç­”æ¡ˆé•¿åº¦
        if len(answer["answer"]) < 50:
            issues.append("ç­”æ¡ˆè¿‡çŸ­,å¯èƒ½ä¸å¤Ÿè¯¦ç»†")

        need_iterate = len(issues) > 0

        return {
            "final_answer": answer["answer"],
            "confidence": answer["confidence"],
            "need_iterate": need_iterate,
            "issues": issues,
            "citations": answer.get("citations", [])
        }

# æµ‹è¯•
if __name__ == "__main__":
    reviewer = ReviewerAgent(confidence_threshold=0.7)

    answer = {
        "answer": "Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶,å¯ä»¥å¹¶è¡Œè®¡ç®—,æ¯”LSTMå¿«å¾ˆå¤š",
        "confidence": 0.85,
        "citations": [1, 2]
    }

    evidence = [
        {"text": "Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶"},
        {"text": "LSTMæ˜¯ä¸²è¡Œçš„"}
    ]

    result = reviewer.review(answer, evidence)
    print(json.dumps(result, indent=2, ensure_ascii=False))
```

---

### 6.4 ç¬¬å››æ­¥ï¼šæ•´åˆç³»ç»Ÿ

```python
# main.py
import os
from agents.planner import PlannerAgent
from agents.retriever import RetrieverAgent
from agents.caption import CaptionAgent
from agents.reasoner import ReasonerAgent
from agents.reviewer import ReviewerAgent
from tools.pdf_parser import PDFParser
from tools.vector_db import VectorDatabase

class SciResearcher:
    """SciResearcherä¸»ç³»ç»Ÿ"""

    def __init__(self):
        # åˆå§‹åŒ–æ‰€æœ‰Agent
        print("æ­£åœ¨åˆå§‹åŒ–Agent...")
        self.planner = PlannerAgent()
        self.retriever = RetrieverAgent()
        self.caption = CaptionAgent()
        self.reasoner = ReasonerAgent()
        self.reviewer = ReviewerAgent()

        # åˆå§‹åŒ–å·¥å…·
        self.pdf_parser = PDFParser()
        self.vector_db = VectorDatabase()

        print("åˆå§‹åŒ–å®Œæˆ!")

    def process_pdf(self, pdf_path):
        """å¤„ç†PDFæ–‡ä»¶å¹¶å»ºç«‹ç´¢å¼•"""
        print(f"\næ­£åœ¨å¤„ç†PDF: {pdf_path}")

        # 1. è§£æPDF
        print("æ­¥éª¤1: è§£æPDF...")
        result = self.pdf_parser.parse(pdf_path)

        # 2. åˆ†å—æ–‡æœ¬
        print("æ­¥éª¤2: åˆ†å—æ–‡æœ¬...")
        chunks = self._chunk_text(result["text"], chunk_size=500)

        # 3. å»ºç«‹å‘é‡ç´¢å¼•
        print("æ­¥éª¤3: å»ºç«‹å‘é‡ç´¢å¼•...")
        self.vector_db.add(chunks)
        self.vector_db.save()

        print(f"å¤„ç†å®Œæˆ! å…±ç´¢å¼•{len(chunks)}ä¸ªæ–‡æœ¬å—")
        return result

    def _chunk_text(self, text, chunk_size=500):
        """å°†é•¿æ–‡æœ¬åˆ†å—"""
        sentences = text.split('.')
        chunks = []
        current_chunk = ""

        for sent in sentences:
            if len(current_chunk) + len(sent) < chunk_size:
                current_chunk += sent + "."
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sent + "."

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def answer_question(self, question, max_iterations=3):
        """å›ç­”ç”¨æˆ·é—®é¢˜"""
        print(f"\nç”¨æˆ·é—®é¢˜: {question}")
        print("="*60)

        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            print(f"\nè¿­ä»£ {iteration}/{max_iterations}")

            # 1. ä»»åŠ¡è§„åˆ’
            print("\næ­¥éª¤1: ä»»åŠ¡è§„åˆ’...")
            tasks = self.planner.plan(question)
            print(f"ç”Ÿæˆçš„å­ä»»åŠ¡: {tasks}")

            # 2. æ£€ç´¢è¯æ®
            print("\næ­¥éª¤2: æ£€ç´¢è¯æ®...")
            retrieval_tasks = tasks.get("retrieval_tasks", [])
            evidence = self.retriever.retrieve(retrieval_tasks)
            print(f"æ£€ç´¢åˆ°{len(evidence)}æ¡è¯æ®")

            # 3. å›¾åƒç†è§£(å¦‚æœæœ‰)
            if tasks.get("image_tasks"):
                print("\næ­¥éª¤3: å›¾åƒç†è§£...")
                for img_task in tasks["image_tasks"]:
                    # TODO: å®ç°å›¾åƒç†è§£é€»è¾‘
                    pass

            # 4. æ¨ç†ç”Ÿæˆ
            print("\næ­¥éª¤4: æ¨ç†ç”Ÿæˆ...")
            answer = self.reasoner.reason(question, evidence)
            print(f"ç½®ä¿¡åº¦: {answer['confidence']:.2f}")

            # 5. ç­”æ¡ˆå®¡æ ¸
            print("\næ­¥éª¤5: ç­”æ¡ˆå®¡æ ¸...")
            review_result = self.reviewer.review(answer, evidence)

            if not review_result["need_iterate"]:
                print("\nå®¡æ ¸é€šè¿‡!")
                return self._format_answer(review_result, evidence)
            else:
                print(f"\nå®¡æ ¸æœªé€šè¿‡,é—®é¢˜: {review_result['issues']}")
                if iteration < max_iterations:
                    print("å‡†å¤‡é‡æ–°æ£€ç´¢...")

        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        print("\nè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°,è¿”å›å½“å‰æœ€ä½³ç­”æ¡ˆ")
        return self._format_answer(review_result, evidence)

    def _format_answer(self, review_result, evidence):
        """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ"""
        output = f"""
{'='*60}
æœ€ç»ˆç­”æ¡ˆ
{'='*60}

{review_result['final_answer']}

ç½®ä¿¡åº¦: {review_result['confidence']:.2%}

å¼•ç”¨è¯æ®:
"""
        for cite_idx in review_result.get("citations", []):
            if cite_idx <= len(evidence):
                output += f"\n[{cite_idx}] {evidence[cite_idx-1]['text'][:200]}...\n"

        return output

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç³»ç»Ÿ
    system = SciResearcher()

    # å¤„ç†PDF
    pdf_path = "./data/pdfs/sample_paper.pdf"
    system.process_pdf(pdf_path)

    # å›ç­”é—®é¢˜
    question = "è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆ?"
    answer = system.answer_question(question)
    print(answer)
```

---

## 7. æäº¤è¦æ±‚

### 7.1 é­”æ­ç¤¾åŒºå‘å¸ƒæ¸…å•

#### å¿…é¡»åŒ…å«çš„æ–‡ä»¶

```
ğŸ“¦ SciResearcher-Release/
â”œâ”€â”€ ğŸ“„ README.md              # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ ğŸ“„ LICENSE                # å¼€æºåè®®(MIT/Apache 2.0)
â”œâ”€â”€ ğŸ“„ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ ğŸ“ agents/                # Agentå®ç°ä»£ç 
â”œâ”€â”€ ğŸ“ tools/                 # å·¥å…·ä»£ç 
â”œâ”€â”€ ğŸ“ docs/                  # æ–‡æ¡£
â”‚   â”œâ”€â”€ å®‰è£…æŒ‡å—.md
â”‚   â”œâ”€â”€ å¿«é€Ÿå¼€å§‹.md
â”‚   â”œâ”€â”€ APIæ–‡æ¡£.md
â”‚   â””â”€â”€ æ¶æ„è¯´æ˜.md
â”œâ”€â”€ ğŸ“ examples/              # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ example1_basic.py
â”‚   â””â”€â”€ example2_advanced.py
â”œâ”€â”€ ğŸ“ experiments/           # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ experiment_report.md
â”‚   â””â”€â”€ benchmark_results.json
â”œâ”€â”€ ğŸ“ data/                  # ç¤ºä¾‹æ•°æ®
â”‚   â””â”€â”€ sample_paper.pdf
â””â”€â”€ ğŸ“ scripts/               # è„šæœ¬
    â”œâ”€â”€ download_models.py
    â””â”€â”€ run_benchmark.py
```

#### README.mdæ¨¡æ¿

```markdown
# SciResearcher: åŸºäºMulti-Agentçš„ç§‘ç ”æ–‡çŒ®æ·±åº¦ç†è§£æ¡†æ¶

[![License](https://img.shields.io/badge/License-MIT-blue.svg)]()
[![Python](https://img.shields.io/badge/Python-3.10+-green.svg)]()

## é¡¹ç›®ç®€ä»‹

SciResearcheræ˜¯é¦–ä¸ªåŸºäºsmolagents + Qwen3ç³»åˆ—æ¨¡å‹ + MinerU2.5çš„ç§‘ç ”æ–‡çŒ®ç ”ç©¶æ¡†æ¶,ä¸“ä¸ºç§‘ç ”å·¥ä½œè€…è®¾è®¡çš„è½»é‡ã€å¼€æºã€å¯éªŒè¯çš„AIåŠ©æ‰‹ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**:
- âœ… å¤šAgentåä½œ: 5ä¸ªæ™ºèƒ½ä½“åˆ†å·¥åˆä½œ
- âœ… å¤šæ¨¡æ€ç†è§£: æ”¯æŒæ–‡å­—ã€å›¾è¡¨ã€å…¬å¼è§£æ
- âœ… å¯éªŒè¯è¾“å‡º: å¼ºåˆ¶å¼•ç”¨+ç½®ä¿¡åº¦è¯„åˆ†
- âœ… å¼€æºå¯å®šåˆ¶: å®Œå…¨å¼€æº,æ˜“äºæ‰©å±•

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install -r requirements.txt
python scripts/download_models.py
```

### åŸºç¡€ä½¿ç”¨

```python
from main import SciResearcher

system = SciResearcher()
system.process_pdf("your_paper.pdf")
answer = system.answer_question("è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆ?")
print(answer)
```

## æ¶æ„è¯´æ˜

[æ’å…¥æ¶æ„å›¾]

## å®éªŒç»“æœ

åœ¨Xä¸ªæµ‹è¯•è®ºæ–‡ä¸Š,å¹³å‡å‡†ç¡®ç‡è¾¾åˆ°Y%,è¯¦è§ [å®éªŒæŠ¥å‘Š](experiments/experiment_report.md)

## å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©,è¯·å¼•ç”¨:
```bibtex
@software{sciresearcher2024,
  title={SciResearcher: Multi-Agent Framework for Scientific Literature Understanding},
  author={Your Name},
  year={2024}
}
```

## å¼€æºåè®®

MIT License
```

### 7.2 å®éªŒæŠ¥å‘Šæ¨¡æ¿

```markdown
# SciResearcher å®éªŒæŠ¥å‘Š

## 1. å®éªŒè®¾ç½®

### 1.1 æ•°æ®é›†

| æ•°æ®é›† | è®ºæ–‡æ•°é‡ | é¢†åŸŸ | æ¥æº |
|-------|---------|------|------|
| arXiv-CS | 50 | è®¡ç®—æœºç§‘å­¦ | arXiv |
| arXiv-Physics | 30 | ç‰©ç† | arXiv |

### 1.2 è¯„ä¼°æŒ‡æ ‡

- **å‡†ç¡®ç‡**: ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„ä¸€è‡´æ€§
- **å¼•ç”¨å‡†ç¡®æ€§**: å¼•ç”¨çš„è¯æ®æ˜¯å¦æ”¯æŒç­”æ¡ˆ
- **ç½®ä¿¡åº¦æ ¡å‡†**: ç½®ä¿¡åº¦ä¸å®é™…å‡†ç¡®æ€§çš„ç›¸å…³æ€§

### 1.3 åŸºçº¿æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| ChatPDF | å•†ä¸šPDFé—®ç­”ç³»ç»Ÿ |
| RAG-Baseline | åŸºç¡€RAGç³»ç»Ÿ |
| **SciResearcher** | æœ¬é¡¹ç›® |

## 2. å®éªŒç»“æœ

### 2.1 æ•´ä½“æ€§èƒ½

| æ–¹æ³• | å‡†ç¡®ç‡ | å¼•ç”¨å‡†ç¡®æ€§ | å¹³å‡ç½®ä¿¡åº¦ |
|------|--------|-----------|-----------|
| ChatPDF | 65.2% | 72.1% | - |
| RAG-Baseline | 71.5% | 78.3% | 0.68 |
| **SciResearcher** | **82.3%** | **89.7%** | **0.81** |

### 2.2 ä¸åŒé—®é¢˜ç±»å‹çš„è¡¨ç°

| é—®é¢˜ç±»å‹ | ChatPDF | RAG-Baseline | SciResearcher |
|---------|---------|--------------|---------------|
| äº‹å®æ€§é—®é¢˜ | 78% | 84% | **91%** |
| å¯¹æ¯”åˆ†æ | 62% | 68% | **79%** |
| æ–¹æ³•ç†è§£ | 58% | 64% | **75%** |

### 2.3 æ¶ˆèå®éªŒ

æµ‹è¯•å„ä¸ªAgentçš„è´¡çŒ®:

| é…ç½® | å‡†ç¡®ç‡ | è¯´æ˜ |
|------|--------|------|
| å®Œæ•´ç³»ç»Ÿ | 82.3% | æ‰€æœ‰Agentå¯ç”¨ |
| -Reviewer | 76.1% | ç§»é™¤å®¡æ ¸Agent |
| -Caption | 78.5% | ç§»é™¤å›¾åƒç†è§£ |
| -Planner | 71.2% | ç§»é™¤ä»»åŠ¡è§„åˆ’ |

**ç»“è®º**: æ¯ä¸ªAgentéƒ½æœ‰æ˜¾è‘—è´¡çŒ®,Plannerçš„è´¡çŒ®æœ€å¤§

## 3. æ¡ˆä¾‹åˆ†æ

### æ¡ˆä¾‹1: æˆåŠŸæ¡ˆä¾‹

**é—®é¢˜**: "Transformeræ¨¡å‹ç›¸æ¯”RNNæœ‰ä»€ä¹ˆä¼˜åŠ¿?"

**SciResearcherç­”æ¡ˆ**:
```
Transformerç›¸æ¯”RNNæœ‰ä»¥ä¸‹ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿:

1. å¹¶è¡Œè®¡ç®—èƒ½åŠ›: Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶,å¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®,
   è€ŒRNNå¿…é¡»ä¸²è¡Œå¤„ç† [è¯æ®1]

2. é•¿è·ç¦»ä¾èµ–: è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç›´æ¥å»ºæ¨¡ä»»æ„è·ç¦»çš„ä¾èµ–å…³ç³»,è€ŒRNNéœ€è¦é€šè¿‡
   å¤šä¸ªæ—¶é—´æ­¥ä¼ é€’ä¿¡æ¯ [è¯æ®2]

3. è®­ç»ƒæ•ˆç‡: ç”±äºå¯å¹¶è¡ŒåŒ–,Transformerçš„è®­ç»ƒé€Ÿåº¦æ˜¾è‘—å¿«äºRNN [è¯æ®3]

ç½®ä¿¡åº¦: 0.92
```

**è¯„ä»·**: âœ… å‡†ç¡®ã€å…¨é¢ã€æœ‰å¼•ç”¨

### æ¡ˆä¾‹2: å¤±è´¥æ¡ˆä¾‹

**é—®é¢˜**: "è¿™ä¸ªç®—æ³•çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘?"

**SciResearcherç­”æ¡ˆ**:
```
è¯æ®ä¸è¶³,æ— æ³•ç¡®å®šæ—¶é—´å¤æ‚åº¦ã€‚
å»ºè®®: è®ºæ–‡å¯èƒ½åœ¨"ç®—æ³•åˆ†æ"æˆ–"å®éªŒ"ç« èŠ‚æåˆ°,è¯·æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚

ç½®ä¿¡åº¦: 0.35
```

**è¯„ä»·**: âš ï¸ è¯šå®åœ°æ‰¿è®¤ä¸çŸ¥é“,ä½†å¯ä»¥æ”¹è¿›æ£€ç´¢ç­–ç•¥

## 4. æ€§èƒ½åˆ†æ

### 4.1 æ¨ç†é€Ÿåº¦

| ç»„ä»¶ | å¹³å‡è€—æ—¶ |
|------|---------|
| PDFè§£æ | 15s |
| å‘é‡ç´¢å¼• | 8s |
| ä»»åŠ¡è§„åˆ’ | 2s |
| è¯æ®æ£€ç´¢ | 3s |
| ç­”æ¡ˆç”Ÿæˆ | 12s |
| ç­”æ¡ˆå®¡æ ¸ | 2s |
| **æ€»è®¡** | **42s** |

### 4.2 èµ„æºå ç”¨

- GPUæ˜¾å­˜: 24GB (A100)
- å†…å­˜: 32GB
- ç£ç›˜: çº¦50GB(æ¨¡å‹+ç´¢å¼•)

## 5. ç»“è®º

SciResearcheråœ¨ç§‘ç ”æ–‡çŒ®é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•,ç‰¹åˆ«æ˜¯åœ¨:
- ç­”æ¡ˆå‡†ç¡®æ€§
- å¼•ç”¨å¯é æ€§
- ç½®ä¿¡åº¦æ ¡å‡†

æœªæ¥æ”¹è¿›æ–¹å‘:
- æ”¯æŒæ›´å¤šè¯­è¨€
- ä¼˜åŒ–æ¨ç†é€Ÿåº¦
- å¢å¼ºå›¾è¡¨ç†è§£èƒ½åŠ›
```

### 7.3 æ¼”ç¤ºè§†é¢‘è„šæœ¬

#### è§†é¢‘ç»“æ„(5-8åˆ†é’Ÿ)

```
00:00-00:30  å¼€åœºä»‹ç»
  - é—®é¢˜èƒŒæ™¯: ç§‘ç ”å·¥ä½œè€…çš„ç—›ç‚¹
  - é¡¹ç›®å®šä½: SciResearcherç®€ä»‹

00:30-01:30  ç³»ç»Ÿæ¼”ç¤º
  - ä¸Šä¼ PDFè®ºæ–‡
  - æå‡ºé—®é¢˜: "è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®?"
  - å±•ç¤ºå¤„ç†è¿‡ç¨‹(å¸¦è¿›åº¦æ¡)
  - æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ(å¸¦å¼•ç”¨+ç½®ä¿¡åº¦)

01:30-03:00  æŠ€æœ¯æ¶æ„
  - 5ä¸ªAgentçš„åˆ†å·¥
  - æ•°æ®æµè½¬è¿‡ç¨‹
  - å…³é”®æŠ€æœ¯ç‚¹(smolagents, Qwen3, MinerU)

03:00-04:30  æ ¸å¿ƒç‰¹æ€§
  - å¤šæ¨¡æ€ç†è§£: å±•ç¤ºå›¾è¡¨åˆ†æ
  - å¯éªŒè¯è¾“å‡º: å±•ç¤ºå¼•ç”¨å’Œç½®ä¿¡åº¦
  - è‡ªæˆ‘æ ¡éªŒ: å±•ç¤ºReviewerå·¥ä½œ

04:30-05:30  å®éªŒç»“æœ
  - ä¸åŸºçº¿å¯¹æ¯”
  - å‡†ç¡®ç‡æå‡
  - æ¡ˆä¾‹å±•ç¤º

05:30-06:00  å¼€æºä¸åº”ç”¨
  - é­”æ­ç¤¾åŒºåœ°å€
  - å¦‚ä½•ä½¿ç”¨
  - æœªæ¥è§„åˆ’

06:00-06:30  æ€»ç»“
```

---

## 8. å¸¸è§é—®é¢˜è§£ç­”

### Q1: æˆ‘æ²¡æœ‰GPUæ€ä¹ˆåŠ?

**A**: å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ¡ˆ:

1. **ä½¿ç”¨CPUç‰ˆæœ¬** (æ…¢ä½†å¯ç”¨):
```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",  # ä½¿ç”¨7Bè€Œé32B
    device_map="cpu",
    load_in_4bit=True  # 4ä½é‡åŒ–
)
```

2. **ä½¿ç”¨äº‘æœåŠ¡**:
- Google Colab (å…è´¹T4 GPU)
- Kaggle (æ¯å‘¨30å°æ—¶GPU)
- é­”æ­ç¤¾åŒºçš„åœ¨çº¿ç¯å¢ƒ

3. **ä½¿ç”¨API**:
```python
# ä½¿ç”¨é­”æ­çš„æ¨¡å‹API
from modelscope import AutoModel
model = AutoModel.from_pretrained("qwen/...", use_api=True)
```

### Q2: MinerUè§£æå¤±è´¥æ€ä¹ˆåŠ?

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹:

```python
# 1. ç¡®è®¤PDFä¸æ˜¯æ‰«æä»¶
from PyPDF2 import PdfReader
reader = PdfReader("test.pdf")
text = reader.pages[0].extract_text()
if not text or len(text) < 10:
    print("å¯èƒ½æ˜¯æ‰«æä»¶,éœ€è¦OCR")

# 2. ä½¿ç”¨å¤‡ç”¨è§£æå™¨
from pdfplumber import PDF
with pdfplumber.open("test.pdf") as pdf:
    text = pdf.pages[0].extract_text()

# 3. è°ƒæ•´MinerUå‚æ•°
pipe = UNIPipe(pdf_bytes, {
    "_pdf_type": "text",  # æˆ– "ocr"
    "lang": "zh"  # æˆ– "en"
})
```

### Q3: å‘é‡æ£€ç´¢æ•ˆæœä¸å¥½?

**A**: ä¼˜åŒ–ç­–ç•¥:

```python
# 1. è°ƒæ•´åˆ†å—å¤§å°
chunks = chunk_text(text, chunk_size=300)  # è¯•è¯•æ›´å°çš„å—

# 2. æ·»åŠ overlap
def chunk_with_overlap(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

# 3. ä½¿ç”¨æ›´å¥½çš„Embeddingæ¨¡å‹
# è¯•è¯•å…¶ä»–æ¨¡å‹:
# - bge-large-zh-v1.5
# - text-embedding-ada-002
```

### Q4: ç­”æ¡ˆæ€»æ˜¯"è¯æ®ä¸è¶³"?

**A**: å¯èƒ½åŸå› å’Œè§£å†³:

```python
# 1. æ£€ç´¢çš„top_kå¤ªå°
evidence = retriever.retrieve(tasks, top_k=10)  # å¢åŠ åˆ°10

# 2. é—®é¢˜å’Œæ–‡æ¡£ä¸åŒ¹é…
# ç¡®ä¿é—®é¢˜æ˜¯å…³äºè¿™ç¯‡è®ºæ–‡çš„

# 3. Revieweré˜ˆå€¼å¤ªä¸¥æ ¼
reviewer = ReviewerAgent(confidence_threshold=0.5)  # é™ä½é˜ˆå€¼

# 4. æ£€æŸ¥ç´¢å¼•æ˜¯å¦æ­£ç¡®
print(f"ç´¢å¼•ä¸­çš„æ–‡æœ¬æ•°: {len(vector_db.texts)}")
```

### Q5: å¦‚ä½•è¯„ä¼°ç³»ç»Ÿæ€§èƒ½?

**A**: å»ºç«‹è¯„ä¼°æµç¨‹:

```python
# åˆ›å»ºè¯„ä¼°æ•°æ®é›†
eval_dataset = [
    {
        "pdf": "paper1.pdf",
        "question": "ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆ?",
        "ground_truth": "æå‡ºäº†Transformeræ¶æ„"
    },
    # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
]

# è¯„ä¼°å‡½æ•°
def evaluate(system, dataset):
    results = []
    for item in dataset:
        system.process_pdf(item["pdf"])
        answer = system.answer_question(item["question"])

        # è®¡ç®—ç›¸ä¼¼åº¦
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(
            None,
            answer.lower(),
            item["ground_truth"].lower()
        ).ratio()

        results.append({
            "question": item["question"],
            "accuracy": similarity,
            "answer": answer
        })

    avg_accuracy = sum(r["accuracy"] for r in results) / len(results)
    return avg_accuracy, results

# è¿è¡Œè¯„ä¼°
accuracy, results = evaluate(system, eval_dataset)
print(f"å¹³å‡å‡†ç¡®ç‡: {accuracy:.2%}")
```

---

## 9. å­¦ä¹ èµ„æº

### 9.1 å®˜æ–¹æ–‡æ¡£

| èµ„æº | é“¾æ¥ | è¯´æ˜ |
|------|------|------|
| **smolagents** | https://github.com/huggingface/smolagents | å®˜æ–¹GitHub |
| **Qwen** | https://github.com/QwenLM/Qwen | Qwenæ¨¡å‹ä»“åº“ |
| **MinerU** | https://github.com/opendatalab/MinerU | PDFè§£æå·¥å…· |
| **é­”æ­ç¤¾åŒº** | https://modelscope.cn | æ¨¡å‹æ‰˜ç®¡å¹³å° |

### 9.2 æ¨èæ•™ç¨‹

#### Multi-Agentå…¥é—¨
- ğŸ“º [Multi-Agentç³»ç»Ÿä»‹ç»](è§†é¢‘é“¾æ¥)
- ğŸ“„ [smolagentså¿«é€Ÿå¼€å§‹](æ–‡æ¡£é“¾æ¥)

#### Qwenæ¨¡å‹ä½¿ç”¨
- ğŸ“º [Qwen3ç³»åˆ—æ¨¡å‹æ•™ç¨‹](è§†é¢‘é“¾æ¥)
- ğŸ“„ [Qwen-VLå›¾åƒç†è§£æŒ‡å—](æ–‡æ¡£é“¾æ¥)

#### RAGç³»ç»Ÿæ„å»º
- ğŸ“º [RAGç³»ç»Ÿä»é›¶å¼€å§‹](è§†é¢‘é“¾æ¥)
- ğŸ“„ [å‘é‡æ•°æ®åº“é€‰æ‹©æŒ‡å—](æ–‡æ¡£é“¾æ¥)

### 9.3 ç›¸å…³è®ºæ–‡

```bibtex
@article{attention2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={NeurIPS},
  year={2017}
}

@article{qwen2024,
  title={Qwen Technical Report},
  author={Qwen Team},
  year={2024}
}
```

### 9.4 ç¤¾åŒºæ”¯æŒ

- ğŸ’¬ **GitHub Issues**: [é¡¹ç›®åœ°å€]/issues
- ğŸ“§ **é‚®ä»¶**: your_email@example.com
- ğŸ’¡ **è®¨è®ºåŒº**: [é­”æ­ç¤¾åŒºè®¨è®ºåŒº]

---

## 10. è¿›é˜¶æ‰©å±•ä¸ä¼˜åŒ–æ€è·¯

### 10.1 æ ¸å¿ƒåŠŸèƒ½ä¼˜åŒ–

#### 10.1.1 æ™ºèƒ½ä»»åŠ¡åˆ†è§£ä¼˜åŒ–

**å½“å‰é—®é¢˜**: Plannerå¯èƒ½ç”Ÿæˆè¿‡äºç®€å•æˆ–å¤æ‚çš„å­ä»»åŠ¡

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
class EnhancedPlannerAgent:
    """å¢å¼ºç‰ˆPlanner,æ”¯æŒåŠ¨æ€ä»»åŠ¡åˆ†è§£"""

    def plan_with_complexity_analysis(self, question):
        """
        æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´åˆ†è§£ç²’åº¦

        å¤æ‚åº¦è¯„ä¼°:
        - ç®€å•é—®é¢˜(äº‹å®æŸ¥è¯¢): 1-2ä¸ªæ£€ç´¢ä»»åŠ¡
        - ä¸­ç­‰é—®é¢˜(å¯¹æ¯”åˆ†æ): 3-4ä¸ªå­ä»»åŠ¡
        - å¤æ‚é—®é¢˜(ç»¼åˆè¯„è¿°): 5+ä¸ªå­ä»»åŠ¡
        """
        # 1. è¯„ä¼°é—®é¢˜å¤æ‚åº¦
        complexity = self._assess_complexity(question)

        # 2. æ ¹æ®å¤æ‚åº¦è°ƒæ•´åˆ†è§£ç­–ç•¥
        if complexity == "simple":
            return self._simple_decompose(question)
        elif complexity == "medium":
            return self._medium_decompose(question)
        else:
            return self._complex_decompose(question)

    def _assess_complexity(self, question):
        """è¯„ä¼°é—®é¢˜å¤æ‚åº¦"""
        indicators = {
            "simple": ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "ä»‹ç»"],
            "medium": ["å¯¹æ¯”", "åˆ†æ", "ä¼˜ç¼ºç‚¹"],
            "complex": ["ç»¼è¿°", "å‘å±•å†ç¨‹", "æœªæ¥è¶‹åŠ¿"]
        }

        for level, keywords in indicators.items():
            if any(kw in question for kw in keywords):
                return level
        return "medium"

    def _complex_decompose(self, question):
        """å¤æ‚é—®é¢˜åˆ†è§£,åŒ…å«æ€ç»´é“¾æ¨ç†"""
        return {
            "phase_1": {
                "goal": "å»ºç«‹åŸºç¡€ç†è§£",
                "tasks": ["æ£€ç´¢æ ¸å¿ƒæ¦‚å¿µ", "ç†è§£å…³é”®æœ¯è¯­"]
            },
            "phase_2": {
                "goal": "æ·±å…¥åˆ†æ",
                "tasks": ["å¯¹æ¯”ä¸åŒæ–¹æ³•", "åˆ†æä¼˜åŠ£åŠ¿"]
            },
            "phase_3": {
                "goal": "ç»¼åˆæ€»ç»“",
                "tasks": ["å½’çº³å…±æ€§", "æç‚¼ç»“è®º"]
            }
        }
```

**é¢„æœŸæ•ˆæœ**: ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡æå‡15-20%

---

#### 10.1.2 æ··åˆæ£€ç´¢ç­–ç•¥

**å½“å‰é—®é¢˜**: å•ä¸€å‘é‡æ£€ç´¢å¯èƒ½é—æ¼å…³é”®ä¿¡æ¯

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç»“åˆç¨€ç–æ£€ç´¢(BM25) + å¯†é›†æ£€ç´¢(å‘é‡) + é‡æ’åº

```python
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨: BM25 + å‘é‡æ£€ç´¢ + é‡æ’åº"""

    def __init__(self):
        self.vector_db = VectorDatabase()
        self.bm25 = None
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    def build_bm25_index(self, texts):
        """æ„å»ºBM25ç´¢å¼•"""
        tokenized = [text.split() for text in texts]
        self.bm25 = BM25Okapi(tokenized)

    def hybrid_search(self, query, top_k=10):
        """
        æ··åˆæ£€ç´¢æµç¨‹:
        1. BM25æ£€ç´¢ â†’ å–top 50
        2. å‘é‡æ£€ç´¢ â†’ å–top 50
        3. åˆå¹¶å»é‡ â†’ å¾—åˆ°å€™é€‰é›†
        4. é‡æ’åº â†’ å–æœ€ç»ˆtop K
        """
        # 1. BM25æ£€ç´¢(å…³é”®è¯åŒ¹é…)
        bm25_scores = self.bm25.get_scores(query.split())
        bm25_top = np.argsort(bm25_scores)[-50:]

        # 2. å‘é‡æ£€ç´¢(è¯­ä¹‰ç›¸ä¼¼)
        vector_results = self.vector_db.search(query, top_k=50)

        # 3. åˆå¹¶å€™é€‰
        candidates = self._merge_candidates(bm25_top, vector_results)

        # 4. é‡æ’åº
        pairs = [[query, cand["text"]] for cand in candidates]
        rerank_scores = self.reranker.predict(pairs)

        # 5. è¿”å›æœ€ç»ˆç»“æœ
        final_results = sorted(
            zip(candidates, rerank_scores),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        return [{"text": r[0]["text"], "score": r[1]} for r in final_results]

    def _merge_candidates(self, bm25_indices, vector_results):
        """åˆå¹¶å¹¶å»é‡"""
        candidates = {}
        for idx in bm25_indices:
            candidates[idx] = {"text": self.texts[idx], "source": "bm25"}
        for vr in vector_results:
            # åŸºäºæ–‡æœ¬å†…å®¹å»é‡
            text_hash = hash(vr["text"])
            if text_hash not in candidates:
                candidates[text_hash] = vr
        return list(candidates.values())
```

**æ£€ç´¢æ€§èƒ½å¯¹æ¯”**:

| æ£€ç´¢æ–¹æ³• | Recall@10 | Precision@10 | MRR |
|---------|-----------|--------------|-----|
| çº¯å‘é‡æ£€ç´¢ | 0.72 | 0.68 | 0.65 |
| çº¯BM25 | 0.65 | 0.71 | 0.62 |
| **æ··åˆæ£€ç´¢** | **0.83** | **0.79** | **0.76** |

**é¢„æœŸæ•ˆæœ**: æ£€ç´¢å‡†ç¡®ç‡æå‡10-15%

---

#### 10.1.3 å›¾è¡¨ç†è§£å¢å¼º

**å½“å‰é—®é¢˜**: ç®€å•çš„å›¾åƒæè¿°ç¼ºä¹ç»“æ„åŒ–ä¿¡æ¯æå–

**ä¼˜åŒ–æ–¹æ¡ˆ**: å›¾è¡¨åˆ†ç±» + ä¸“é—¨æå–

```python
class EnhancedCaptionAgent:
    """å¢å¼ºå›¾è¡¨ç†è§£Agent"""

    def __init__(self):
        self.vl_model = load_qwen_vl()
        self.chart_classifier = self._load_chart_classifier()

    def understand_figure(self, image_path, context):
        """
        æ™ºèƒ½å›¾è¡¨ç†è§£:
        1. åˆ†ç±»å›¾è¡¨ç±»å‹
        2. é’ˆå¯¹æ€§æå–ä¿¡æ¯
        3. ç»“æ„åŒ–è¾“å‡º
        """
        # 1. å›¾è¡¨åˆ†ç±»
        chart_type = self._classify_chart(image_path)

        # 2. é’ˆå¯¹æ€§æå–
        if chart_type == "line_chart":
            info = self._extract_line_chart(image_path)
        elif chart_type == "bar_chart":
            info = self._extract_bar_chart(image_path)
        elif chart_type == "architecture_diagram":
            info = self._extract_architecture(image_path)
        elif chart_type == "table":
            info = self._extract_table(image_path)
        else:
            info = self._general_caption(image_path)

        # 3. ç»“åˆä¸Šä¸‹æ–‡ç†è§£
        structured_output = self._contextualize(info, context)

        return structured_output

    def _extract_line_chart(self, image_path):
        """æå–æŠ˜çº¿å›¾æ•°æ®"""
        prompt = """
åˆ†æè¿™ä¸ªæŠ˜çº¿å›¾,æå–:
1. Xè½´æ ‡ç­¾å’ŒèŒƒå›´
2. Yè½´æ ‡ç­¾å’ŒèŒƒå›´
3. æ¯æ¡çº¿çš„åç§°å’Œè¶‹åŠ¿
4. å…³é”®æ•°æ®ç‚¹(æœ€é«˜ç‚¹ã€æœ€ä½ç‚¹ã€äº¤å‰ç‚¹)
5. æ•´ä½“è¶‹åŠ¿æè¿°

ä»¥JSONæ ¼å¼è¿”å›ã€‚
"""
        result = self.vl_model.chat(image_path, prompt)
        return json.loads(result)

    def _extract_architecture(self, image_path):
        """æå–æ¶æ„å›¾ä¿¡æ¯"""
        prompt = """
åˆ†æè¿™ä¸ªæ¶æ„å›¾,æå–:
1. ä¸»è¦ç»„ä»¶åˆ—è¡¨
2. ç»„ä»¶ä¹‹é—´çš„è¿æ¥å…³ç³»
3. æ•°æ®æµå‘
4. å±‚æ¬¡ç»“æ„

ä»¥JSONæ ¼å¼è¿”å›ã€‚
"""
        result = self.vl_model.chat(image_path, prompt)
        return json.loads(result)

    def _classify_chart(self, image_path):
        """åˆ†ç±»å›¾è¡¨ç±»å‹"""
        # å¯ä»¥ç”¨CLIPæˆ–ç®€å•çš„åˆ†ç±»å™¨
        types = [
            "line_chart", "bar_chart", "scatter_plot",
            "architecture_diagram", "table", "formula", "other"
        ]
        # ç®€åŒ–ç¤ºä¾‹
        return "line_chart"  # å®é™…åº”è¯¥ç”¨æ¨¡å‹åˆ†ç±»
```

**ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹**:

```json
{
  "chart_type": "line_chart",
  "title": "æ¨¡å‹æ€§èƒ½å¯¹æ¯”",
  "x_axis": {
    "label": "è®­ç»ƒè½®æ•°",
    "range": [0, 100]
  },
  "y_axis": {
    "label": "å‡†ç¡®ç‡(%)",
    "range": [0, 100]
  },
  "lines": [
    {
      "name": "Transformer",
      "trend": "ä¸Šå‡",
      "key_points": {
        "start": {"x": 0, "y": 65},
        "peak": {"x": 80, "y": 92},
        "end": {"x": 100, "y": 91}
      }
    },
    {
      "name": "LSTM",
      "trend": "ç¼“æ…¢ä¸Šå‡",
      "key_points": {
        "start": {"x": 0, "y": 60},
        "peak": {"x": 100, "y": 85},
        "end": {"x": 100, "y": 85}
      }
    }
  ],
  "insights": [
    "Transformeråœ¨40è½®åè¶…è¿‡LSTM",
    "Transformeræœ€ç»ˆæ€§èƒ½æå‡7ä¸ªç™¾åˆ†ç‚¹"
  ]
}
```

**é¢„æœŸæ•ˆæœ**: å›¾è¡¨ç†è§£å‡†ç¡®ç‡æå‡20-30%

---

### 10.2 å…¨æ–°åŠŸèƒ½æ‰©å±•

#### 10.2.1 å¤šæ–‡æ¡£ç»¼è¿°

```python
class MultiDocSynthesizer:
    """å¤šæ–‡æ¡£ç»¼è¿°Agent"""

    def synthesize_survey(self, pdf_paths, research_question):
        """
        ç”Ÿæˆæ–‡çŒ®ç»¼è¿°

        è¾“å‡ºç»“æ„:
        1. ç ”ç©¶é—®é¢˜æ¦‚è¿°
        2. ä¸»è¦æ–¹æ³•åˆ†ç±»
        3. æ–¹æ³•å¯¹æ¯”åˆ†æ
        4. ç ”ç©¶è¶‹åŠ¿
        5. æœªæ¥æ–¹å‘
        """
        # 1. å¤„ç†æ‰€æœ‰æ–‡æ¡£
        all_papers = []
        for pdf in pdf_paths:
            paper_info = self._process_paper(pdf)
            all_papers.append(paper_info)

        # 2. ä¸»é¢˜èšç±»
        clusters = self._cluster_papers(all_papers, research_question)

        # 3. ç”Ÿæˆç»¼è¿°
        survey = {
            "overview": self._generate_overview(research_question),
            "methodology_taxonomy": self._build_taxonomy(clusters),
            "comparative_analysis": self._compare_methods(clusters),
            "timeline": self._analyze_timeline(all_papers),
            "trends": self._identify_trends(all_papers),
            "future_directions": self._suggest_future_work(clusters)
        }

        return self._format_survey(survey)

    def _cluster_papers(self, papers, question):
        """æŒ‰æ–¹æ³•è®ºèšç±»"""
        # ä½¿ç”¨ä¸»é¢˜æ¨¡å‹æˆ–embeddingèšç±»
        from sklearn.cluster import KMeans

        embeddings = [self._embed(p["abstract"]) for p in papers]
        kmeans = KMeans(n_clusters=5)
        labels = kmeans.fit_predict(embeddings)

        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(papers[i])

        return clusters

    def _compare_methods(self, clusters):
        """å¯¹æ¯”ä¸åŒæ–¹æ³•"""
        comparison = []
        for cluster_id, papers in clusters.items():
            method_name = self._extract_method_name(papers)

            comparison.append({
                "method": method_name,
                "papers_count": len(papers),
                "key_papers": [p["title"] for p in papers[:3]],
                "strengths": self._extract_strengths(papers),
                "limitations": self._extract_limitations(papers),
                "performance": self._extract_performance(papers)
            })

        return comparison
```

**è¾“å‡ºç¤ºä¾‹**: è‡ªåŠ¨ç”Ÿæˆçš„æ–‡çŒ®ç»¼è¿°

```markdown
# åŸºäºTransformerçš„æœºå™¨ç¿»è¯‘æ–¹æ³•ç»¼è¿°

## 1. ç ”ç©¶æ¦‚è¿°
æœ¬ç»¼è¿°åˆ†æäº†2017-2024å¹´é—´50ç¯‡å…³äºTransformeråœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„åº”ç”¨è®ºæ–‡...

## 2. æ–¹æ³•åˆ†ç±»

### 2.1 åŸºç¡€Transformeræ”¹è¿› (15ç¯‡)
- **ä»£è¡¨æ€§å·¥ä½œ**: [Vaswani et al., 2017], [Wang et al., 2019]
- **æ ¸å¿ƒæ€æƒ³**: æ”¹è¿›æ³¨æ„åŠ›æœºåˆ¶,æå‡æ¨¡å‹æ•ˆç‡
- **æ€§èƒ½**: BLEUæå‡2-5åˆ†

### 2.2 é¢„è®­ç»ƒæ–¹æ³• (20ç¯‡)
- **ä»£è¡¨æ€§å·¥ä½œ**: [mBART, mT5, NLLB]
- **æ ¸å¿ƒæ€æƒ³**: å¤§è§„æ¨¡å¤šè¯­è¨€é¢„è®­ç»ƒ
- **æ€§èƒ½**: BLEUæå‡5-10åˆ†

## 3. å¯¹æ¯”åˆ†æ
[è‡ªåŠ¨ç”Ÿæˆçš„è¡¨æ ¼å¯¹æ¯”]

## 4. ç ”ç©¶è¶‹åŠ¿
- 2017-2019: æ¶æ„åˆ›æ–°æœŸ
- 2020-2022: é¢„è®­ç»ƒçˆ†å‘æœŸ
- 2023-2024: é«˜æ•ˆå¾®è°ƒæœŸ

## 5. æœªæ¥æ–¹å‘
1. ä½èµ„æºè¯­è¨€ç¿»è¯‘
2. é¢†åŸŸè‡ªé€‚åº”
3. å¤šæ¨¡æ€ç¿»è¯‘
```

---

#### 10.2.2 å®éªŒè®¾è®¡å»ºè®®

```python
class ExperimentDesigner:
    """å®éªŒè®¾è®¡é¡¾é—®Agent"""

    def design_experiment(self, research_question, context):
        """
        åŸºäºç ”ç©¶é—®é¢˜è®¾è®¡å®éªŒ

        åŒ…å«:
        1. ç ”ç©¶å‡è®¾
        2. å®éªŒè®¾è®¡
        3. è¯„ä¼°æŒ‡æ ‡
        4. ç»Ÿè®¡æ–¹æ³•
        5. é¢„æœŸç»“æœ
        """
        design = {
            "hypothesis": self._formulate_hypothesis(research_question),
            "experimental_design": self._design_experiment(research_question),
            "evaluation_metrics": self._select_metrics(research_question),
            "baseline_methods": self._suggest_baselines(research_question),
            "dataset_requirements": self._specify_datasets(research_question),
            "statistical_analysis": self._plan_statistics(research_question),
            "expected_outcomes": self._predict_outcomes(research_question)
        }

        return design

    def _design_experiment(self, question):
        """è®¾è®¡å®éªŒæ–¹æ¡ˆ"""
        return {
            "control_group": {
                "description": "åŸºçº¿æ–¹æ³•",
                "setup": "ä½¿ç”¨æ ‡å‡†LSTMæ¨¡å‹"
            },
            "experimental_group": {
                "description": "æå‡ºçš„æ–¹æ³•",
                "setup": "ä½¿ç”¨æ”¹è¿›çš„Transformeræ¨¡å‹"
            },
            "variables": {
                "independent": ["æ¨¡å‹æ¶æ„"],
                "dependent": ["ç¿»è¯‘è´¨é‡(BLEU)", "æ¨ç†é€Ÿåº¦"],
                "controlled": ["è®­ç»ƒæ•°æ®", "è¶…å‚æ•°"]
            },
            "sample_size": "æ¯ç»„è‡³å°‘1000ä¸ªæµ‹è¯•æ ·æœ¬",
            "randomization": "éšæœºåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†"
        }

    def _select_metrics(self, question):
        """é€‰æ‹©è¯„ä¼°æŒ‡æ ‡"""
        # åŸºäºé¢†åŸŸæ™ºèƒ½æ¨èæŒ‡æ ‡
        domain = self._identify_domain(question)

        metrics_by_domain = {
            "machine_translation": {
                "primary": ["BLEU", "chrF"],
                "secondary": ["METEOR", "COMET"],
                "human_eval": ["Adequacy", "Fluency"]
            },
            "text_classification": {
                "primary": ["Accuracy", "F1"],
                "secondary": ["Precision", "Recall"],
                "additional": ["Confusion Matrix"]
            }
        }

        return metrics_by_domain.get(domain, {})
```

**å®éªŒè®¾è®¡è¾“å‡º**:

```yaml
ç ”ç©¶é—®é¢˜: Transformeræ˜¯å¦ä¼˜äºLSTMåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Š?

ç ”ç©¶å‡è®¾:
  H0: Transformerå’ŒLSTMçš„ç¿»è¯‘è´¨é‡æ— æ˜¾è‘—å·®å¼‚
  H1: Transformerçš„ç¿»è¯‘è´¨é‡æ˜¾è‘—ä¼˜äºLSTM (p < 0.05)

å®éªŒè®¾è®¡:
  ç±»å‹: å¯¹ç…§å®éªŒ
  å¯¹ç…§ç»„: LSTM baseline
  å®éªŒç»„: Transformeræ¨¡å‹
  æ ·æœ¬é‡: æ¯ç»„2000ä¸ªå¥å­å¯¹

è¯„ä¼°æŒ‡æ ‡:
  ä¸»è¦æŒ‡æ ‡: BLEU, chrF
  æ¬¡è¦æŒ‡æ ‡: æ¨ç†é€Ÿåº¦(å¥å­/ç§’)
  äººå·¥è¯„ä¼°: æŠ½æ ·200ä¸ªæ ·æœ¬è¿›è¡ŒAdequacyå’ŒFluencyæ‰“åˆ†

æ•°æ®é›†:
  è®­ç»ƒé›†: WMT14 EN-DE (450ä¸‡å¥å¯¹)
  éªŒè¯é›†: newstest2013 (3000å¥å¯¹)
  æµ‹è¯•é›†: newstest2014 (3003å¥å¯¹)

ç»Ÿè®¡åˆ†æ:
  æ˜¾è‘—æ€§æ£€éªŒ: é…å¯¹tæ£€éªŒ (p < 0.05)
  æ•ˆåº”é‡: Cohen's d
  ç½®ä¿¡åŒºé—´: 95% CI

é¢„æœŸç»“æœ:
  BLEUæå‡: 3-5åˆ†
  æ¨ç†é€Ÿåº¦: æå‡2-3å€
  ç»Ÿè®¡æ˜¾è‘—æ€§: é¢„è®¡p < 0.01
```

---

#### 10.2.3 ç ”ç©¶ç©ºç™½æ¢æµ‹

```python
class GapDetector:
    """ç ”ç©¶ç©ºç™½æ¢æµ‹Agent"""

    def detect_research_gaps(self, papers, domain):
        """
        å‘ç°ç ”ç©¶ç©ºç™½

        åˆ†æç»´åº¦:
        1. æ–¹æ³•è®ºç©ºç™½
        2. åº”ç”¨åœºæ™¯ç©ºç™½
        3. æ•°æ®é›†ç©ºç™½
        4. è¯„ä¼°æŒ‡æ ‡ç©ºç™½
        """
        gaps = {
            "methodology_gaps": self._find_method_gaps(papers),
            "application_gaps": self._find_application_gaps(papers),
            "dataset_gaps": self._find_dataset_gaps(papers),
            "evaluation_gaps": self._find_evaluation_gaps(papers)
        }

        # è¯„ä¼°ç ”ç©¶æœºä¼š
        opportunities = self._rank_opportunities(gaps)

        return {
            "gaps": gaps,
            "opportunities": opportunities,
            "recommendations": self._generate_recommendations(opportunities)
        }

    def _find_method_gaps(self, papers):
        """å‘ç°æ–¹æ³•è®ºç©ºç™½"""
        # 1. æå–æ‰€æœ‰ä½¿ç”¨çš„æ–¹æ³•
        methods = []
        for paper in papers:
            methods.extend(self._extract_methods(paper))

        # 2. æ„å»ºæ–¹æ³•ç©ºé—´
        method_space = self._build_method_space(domain)

        # 3. å‘ç°æœªè¢«æ¢ç´¢çš„ç»„åˆ
        unexplored = []
        for combo in method_space["possible_combinations"]:
            if not self._is_explored(combo, methods):
                unexplored.append(combo)

        return unexplored

    def _rank_opportunities(self, gaps):
        """è¯„ä¼°ç ”ç©¶æœºä¼š"""
        opportunities = []

        for gap_type, gap_list in gaps.items():
            for gap in gap_list:
                score = self._calculate_opportunity_score(gap)
                opportunities.append({
                    "gap": gap,
                    "type": gap_type,
                    "score": score,
                    "rationale": self._explain_score(gap, score)
                })

        # æ’åº
        opportunities.sort(key=lambda x: x["score"], reverse=True)
        return opportunities[:10]  # Top 10æœºä¼š

    def _calculate_opportunity_score(self, gap):
        """è®¡ç®—ç ”ç©¶æœºä¼šåˆ†æ•°"""
        # ç»¼åˆè€ƒè™‘:
        # 1. åˆ›æ–°æ€§ (0-1)
        # 2. å¯è¡Œæ€§ (0-1)
        # 3. å½±å“åŠ› (0-1)
        # 4. èµ„æºéœ€æ±‚ (0-1, è¶Šä½è¶Šå¥½)

        novelty = self._assess_novelty(gap)
        feasibility = self._assess_feasibility(gap)
        impact = self._assess_impact(gap)
        resources = 1 - self._assess_resource_requirements(gap)

        # åŠ æƒå¹³å‡
        score = (
            0.35 * novelty +
            0.25 * feasibility +
            0.30 * impact +
            0.10 * resources
        )

        return score
```

**ç ”ç©¶ç©ºç™½æŠ¥å‘Šç¤ºä¾‹**:

```markdown
# æœºå™¨ç¿»è¯‘é¢†åŸŸç ”ç©¶ç©ºç™½åˆ†ææŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦
åŸºäºå¯¹150ç¯‡è®ºæ–‡(2020-2024)çš„åˆ†æ,è¯†åˆ«å‡º12ä¸ªé«˜ä»·å€¼ç ”ç©¶æœºä¼šã€‚

## Top 5 ç ”ç©¶æœºä¼š

### 1. ä½èµ„æºè¯­è¨€çš„å°‘æ ·æœ¬ç¿»è¯‘ (æœºä¼šåˆ†æ•°: 0.87)

**ç©ºç™½æè¿°**:
- å½“å‰ç ”ç©¶é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€å¯¹(EN-DE, EN-ZH)
- ä»…3%çš„è®ºæ–‡å…³æ³¨ä½èµ„æºè¯­è¨€(<1Må¥å¯¹)
- å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•åœ¨ç¿»è¯‘é¢†åŸŸåº”ç”¨ä¸è¶³

**åˆ›æ–°æ€§**: 0.90 (é«˜)
- ç°æœ‰å·¥ä½œè¾ƒå°‘,åˆ›æ–°ç©ºé—´å¤§

**å¯è¡Œæ€§**: 0.85 (é«˜)
- å¯å¤ç”¨é¢„è®­ç»ƒæ¨¡å‹
- å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯æˆç†Ÿ

**å½±å“åŠ›**: 0.92 (é«˜)
- æœåŠ¡å…¨çƒ70%+è¯­è¨€
- ç¤¾ä¼šä»·å€¼æ˜¾è‘—

**èµ„æºéœ€æ±‚**: 0.80 (ä¸­ç­‰)
- éœ€è¦æ”¶é›†å°‘é‡é«˜è´¨é‡æ•°æ®
- GPUéœ€æ±‚é€‚ä¸­(å•å¡å¯è®­ç»ƒ)

**å»ºè®®ç ”ç©¶æ–¹å‘**:
1. ç»“åˆå…ƒå­¦ä¹ çš„å°‘æ ·æœ¬ç¿»è¯‘
2. è·¨è¯­è¨€è¿ç§»å­¦ä¹ 
3. ä¸»åŠ¨å­¦ä¹ æ•°æ®é€‰æ‹©

**ç›¸å…³æ•°æ®é›†**:
- FLORES-200 (ä½èµ„æºè¯„æµ‹)
- AmericasNLP (ç¾æ´²åœŸè‘—è¯­è¨€)

**æ½œåœ¨åˆä½œè€…**:
- ç ”ç©¶ä½èµ„æºNLPçš„å®éªŒå®¤
- è¯­è¨€ä¿æŠ¤ç»„ç»‡

---

### 2. å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ (æœºä¼šåˆ†æ•°: 0.82)

**ç©ºç™½æè¿°**:
- ç»“åˆå›¾åƒä¸Šä¸‹æ–‡çš„ç¿»è¯‘ç ”ç©¶ä¸è¶³(<5%è®ºæ–‡)
- è§†è§‰æ¥åœ°(visual grounding)åœ¨ç¿»è¯‘ä¸­åº”ç”¨æœ‰é™
- ç¼ºä¹å¤§è§„æ¨¡å¤šæ¨¡æ€ç¿»è¯‘æ•°æ®é›†

**åˆ›æ–°æ€§**: 0.88
**å¯è¡Œæ€§**: 0.75
**å½±å“åŠ›**: 0.85
**èµ„æºéœ€æ±‚**: 0.70

[è¯¦ç»†åˆ†æ...]

---

## æ–¹æ³•è®ºç©ºç™½çŸ©é˜µ

| æ–¹æ³•ç»´åº¦ | å·²æ¢ç´¢ | æœªæ¢ç´¢ | æœºä¼šè¯„åˆ† |
|---------|--------|--------|---------|
| æ¶æ„åˆ›æ–° | 85% | 15% | ä¸­ |
| é¢„è®­ç»ƒç­–ç•¥ | 90% | 10% | ä½ |
| é¢†åŸŸè‡ªé€‚åº” | 40% | 60% | **é«˜** |
| å¤šè¯­è¨€å­¦ä¹  | 70% | 30% | ä¸­ |
| é«˜æ•ˆæ¨ç† | 50% | 50% | **é«˜** |

## åº”ç”¨åœºæ™¯ç©ºç™½

æœªå……åˆ†ç ”ç©¶çš„åœºæ™¯:
1. âœ… å®æ—¶è¯­éŸ³ç¿»è¯‘ (ä»…8ç¯‡)
2. âœ… ä»£ç ç¿»è¯‘ (ä»…12ç¯‡)
3. âœ… åŒ»ç–—æ–‡æ¡£ç¿»è¯‘ (ä»…5ç¯‡)
4. âœ… æ³•å¾‹æ–‡æœ¬ç¿»è¯‘ (ä»…3ç¯‡)

## æ•°æ®é›†ç©ºç™½

ç¼ºå¤±çš„æ•°æ®é›†ç±»å‹:
- ä½èµ„æºè¯­è¨€å¯¹ (>100ä¸ªè¯­è¨€å¯¹)
- é¢†åŸŸç‰¹å®šæ•°æ® (åŒ»ç–—ã€æ³•å¾‹ã€æŠ€æœ¯)
- å¤šæ¨¡æ€ç¿»è¯‘æ•°æ®
- å¯¹è¯ç¿»è¯‘æ•°æ®

## å»ºè®®è¡ŒåŠ¨è®¡åˆ’

### çŸ­æœŸ(3-6ä¸ªæœˆ):
1. è°ƒç ”ä½èµ„æºç¿»è¯‘æ–¹æ³•
2. æ”¶é›†å°‘æ ·æœ¬å­¦ä¹ æ•°æ®
3. å®ç°baselineæ¨¡å‹

### ä¸­æœŸ(6-12ä¸ªæœˆ):
1. å¼€å‘åˆ›æ–°æ¨¡å‹
2. è¿›è¡Œå®éªŒéªŒè¯
3. æ’°å†™è®ºæ–‡æŠ•ç¨¿

### é•¿æœŸ(1-2å¹´):
1. æ‰©å±•åˆ°æ›´å¤šè¯­è¨€
2. å¼€æºå·¥å…·å’Œæ•°æ®
3. æ¨åŠ¨ç¤¾åŒºé‡‡ç”¨
```

---

#### 10.2.4 Hugging Face Daily Papersé›†æˆ

```python
class HFDailyPaperMonitor:
    """Hugging Face Daily Papersç›‘æ§"""

    def __init__(self):
        self.hf_api = "https://huggingface.co/papers"
        self.db = VectorDatabase(index_path="./data/daily_papers.faiss")

    def fetch_daily_papers(self):
        """è·å–æ¯æ—¥è®ºæ–‡"""
        import requests
        response = requests.get(f"{self.hf_api}/daily")
        papers = response.json()

        new_papers = []
        for paper in papers:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not self._is_processed(paper["id"]):
                new_papers.append(paper)

        return new_papers

    def process_new_papers(self):
        """å¤„ç†æ–°è®ºæ–‡"""
        papers = self.fetch_daily_papers()

        for paper in papers:
            # 1. ä¸‹è½½PDF
            pdf_path = self._download_pdf(paper["pdf_url"])

            # 2. è§£æ
            parsed = self.pdf_parser.parse(pdf_path)

            # 3. ç´¢å¼•
            chunks = self._chunk_text(parsed["text"])
            self.db.add(chunks)

            # 4. æå–å…³é”®ä¿¡æ¯
            summary = self._summarize_paper(parsed)

            # 5. ä¿å­˜å…ƒæ•°æ®
            self._save_metadata(paper["id"], summary)

        self.db.save()
        return len(papers)

    def smart_recommendation(self, user_interests):
        """æ™ºèƒ½æ¨èè®ºæ–‡"""
        # åŸºäºç”¨æˆ·å…´è¶£æ¨è
        relevant_papers = []

        for interest in user_interests:
            # æ£€ç´¢ç›¸å…³è®ºæ–‡
            results = self.db.search(interest, top_k=5)
            relevant_papers.extend(results)

        # å»é‡å’Œæ’åº
        unique_papers = self._deduplicate(relevant_papers)

        return unique_papers[:10]

# ä½¿ç”¨ç¤ºä¾‹
monitor = HFDailyPaperMonitor()

# å®šæ—¶ä»»åŠ¡: æ¯å¤©æ—©ä¸Š8ç‚¹è¿è¡Œ
def daily_update():
    new_count = monitor.process_new_papers()
    print(f"å¤„ç†äº†{new_count}ç¯‡æ–°è®ºæ–‡")

# è·å–æ¨è
recommendations = monitor.smart_recommendation([
    "Transformer",
    "æœºå™¨ç¿»è¯‘",
    "å¤šæ¨¡æ€å­¦ä¹ "
])
```

---

#### 10.2.5 MCPå·¥å…·å°è£…

å°†æ¯ä¸ªAgentå°è£…ä¸ºMCP Server,æ”¯æŒè·¨åº”ç”¨è°ƒç”¨:

```python
# mcp_servers/retriever_server.py
from mcp.server import Server
from agents.retriever import RetrieverAgent

class RetrieverMCPServer(Server):
    """Retriever Agentçš„MCP Serverå°è£…"""

    def __init__(self):
        super().__init__("retriever-agent")
        self.retriever = RetrieverAgent()

        # æ³¨å†Œå·¥å…·
        self.register_tool(
            name="retrieve_evidence",
            description="ä»ç§‘ç ”æ–‡çŒ®ä¸­æ£€ç´¢ç›¸å…³è¯æ®",
            parameters={
                "query": {"type": "string", "description": "æ£€ç´¢æŸ¥è¯¢"},
                "top_k": {"type": "integer", "default": 5}
            },
            handler=self.retrieve
        )

    async def retrieve(self, query: str, top_k: int = 5):
        """æ£€ç´¢å¤„ç†å‡½æ•°"""
        results = self.retriever.retrieve([query], top_k)
        return {
            "evidence": results,
            "count": len(results)
        }

# å¯åŠ¨MCP Server
if __name__ == "__main__":
    server = RetrieverMCPServer()
    server.run(host="localhost", port=8001)
```

**MCP Serveræ¶æ„**:

```
SciResearcher MCP Ecosystem
â”œâ”€â”€ retriever-mcp (ç«¯å£8001)
â”‚   â””â”€â”€ retrieve_evidence
â”œâ”€â”€ caption-mcp (ç«¯å£8002)
â”‚   â””â”€â”€ understand_figure
â”œâ”€â”€ reasoner-mcp (ç«¯å£8003)
â”‚   â””â”€â”€ generate_answer
â”œâ”€â”€ planner-mcp (ç«¯å£8004)
â”‚   â””â”€â”€ decompose_task
â””â”€â”€ reviewer-mcp (ç«¯å£8005)
    â””â”€â”€ review_answer
```

**è·¨åº”ç”¨è°ƒç”¨ç¤ºä¾‹**:

```python
# åœ¨Claude Codeä¸­è°ƒç”¨
from mcp.client import MCPClient

# è¿æ¥åˆ°Retriever MCP
retriever_client = MCPClient("http://localhost:8001")

# è°ƒç”¨æ£€ç´¢å·¥å…·
evidence = retriever_client.call_tool(
    "retrieve_evidence",
    query="Transformerçš„æ³¨æ„åŠ›æœºåˆ¶",
    top_k=5
)

print(evidence)
```

---

### 10.3 æ€§èƒ½ä¼˜åŒ–æ€è·¯

#### 10.3.1 æ¨ç†åŠ é€Ÿ

**ä¼˜åŒ–æ–¹å‘**:

| ä¼˜åŒ–æŠ€æœ¯ | é¢„æœŸåŠ é€Ÿ | å®ç°éš¾åº¦ | è´¨é‡å½±å“ |
|---------|---------|---------|---------|
| **æ¨¡å‹é‡åŒ–(INT8)** | 2-3x | ä½ | <2% |
| **FlashAttention** | 2-4x | ä¸­ | æ—  |
| **KV Cacheä¼˜åŒ–** | 1.5-2x | ä¸­ | æ—  |
| **æ‰¹å¤„ç†æ¨ç†** | 3-5x | ä½ | æ—  |
| **è’¸é¦å°æ¨¡å‹** | 5-10x | é«˜ | 5-10% |

**å®ç°ç¤ºä¾‹**:

```python
from transformers import AutoModelForCausalLM
import torch

# 1. INT8é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-32B-Instruct",
    device_map="auto",
    load_in_8bit=True  # INT8é‡åŒ–
)

# 2. FlashAttention
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-32B-Instruct",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16
)

# 3. æ‰¹å¤„ç†æ¨ç†
def batch_inference(queries, batch_size=4):
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        outputs = model.generate(batch, max_length=512)
        results.extend(outputs)
    return results
```

**æ€§èƒ½å¯¹æ¯”**:

```
æ¨ç†é€Ÿåº¦æµ‹è¯• (å•æ¬¡æŸ¥è¯¢):
â”œâ”€â”€ åŸå§‹æ¨¡å‹(FP16): 42s
â”œâ”€â”€ INT8é‡åŒ–: 18s (2.3xåŠ é€Ÿ)
â”œâ”€â”€ FlashAttention: 15s (2.8xåŠ é€Ÿ)
â”œâ”€â”€ æ‰¹å¤„ç†(batch=4): 10s/query (4.2xåŠ é€Ÿ)
â””â”€â”€ å…¨éƒ¨ä¼˜åŒ–: 6s (7xåŠ é€Ÿ)
```

---

#### 10.3.2 ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import hashlib

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir="./cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def cache_pdf_parsing(self, pdf_path):
        """ç¼“å­˜PDFè§£æç»“æœ"""
        # è®¡ç®—PDFçš„hash
        with open(pdf_path, "rb") as f:
            pdf_hash = hashlib.md5(f.read()).hexdigest()

        cache_file = f"{self.cache_dir}/{pdf_hash}.json"

        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(cache_file):
            with open(cache_file, "r") as f:
                return json.load(f)

        # è§£æå¹¶ç¼“å­˜
        result = self.pdf_parser.parse(pdf_path)
        with open(cache_file, "w") as f:
            json.dump(result, f)

        return result

    @lru_cache(maxsize=1000)
    def cache_embedding(self, text):
        """ç¼“å­˜Embeddingç»“æœ"""
        return self.embed_model.encode(text)

    def cache_reasoning(self, question, evidence_hash):
        """ç¼“å­˜æ¨ç†ç»“æœ"""
        cache_key = f"{hash(question)}_{evidence_hash}"
        cache_file = f"{self.cache_dir}/reasoning/{cache_key}.json"

        if os.path.exists(cache_file):
            with open(cache_file, "r") as f:
                return json.load(f)

        # ç”Ÿæˆå¹¶ç¼“å­˜
        result = self.reasoner.reason(question, evidence)
        with open(cache_file, "w") as f:
            json.dump(result, f)

        return result
```

**ç¼“å­˜å‘½ä¸­ç‡ä¼˜åŒ–**:

```
ç¼“å­˜ç­–ç•¥æ•ˆæœ:
â”œâ”€â”€ PDFè§£æç¼“å­˜å‘½ä¸­ç‡: 85% (é‡å¤æ–‡æ¡£)
â”œâ”€â”€ Embeddingç¼“å­˜å‘½ä¸­ç‡: 60% (å¸¸è§æŸ¥è¯¢)
â”œâ”€â”€ æ¨ç†ç¼“å­˜å‘½ä¸­ç‡: 40% (ç›¸ä¼¼é—®é¢˜)
â””â”€â”€ æ•´ä½“æ—¶é—´èŠ‚çœ: 65%
```

---

#### 10.3.3 å¹¶è¡Œå¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class ParallelProcessor:
    """å¹¶è¡Œå¤„ç†å¼•æ“"""

    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    async def parallel_retrieve(self, tasks):
        """å¹¶è¡Œæ£€ç´¢å¤šä¸ªä»»åŠ¡"""
        loop = asyncio.get_event_loop()

        # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
        futures = [
            loop.run_in_executor(
                self.executor,
                self.retriever.retrieve,
                [task]
            )
            for task in tasks
        ]

        results = await asyncio.gather(*futures)
        return results

    async def parallel_caption(self, image_paths):
        """å¹¶è¡Œç†è§£å¤šä¸ªå›¾ç‰‡"""
        loop = asyncio.get_event_loop()

        futures = [
            loop.run_in_executor(
                self.executor,
                self.caption_agent.caption,
                img_path,
                "åˆ†æè¿™ä¸ªå›¾è¡¨"
            )
            for img_path in image_paths
        ]

        results = await asyncio.gather(*futures)
        return results

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    processor = ParallelProcessor(max_workers=4)

    # å¹¶è¡Œæ£€ç´¢5ä¸ªä»»åŠ¡
    tasks = ["ä»»åŠ¡1", "ä»»åŠ¡2", "ä»»åŠ¡3", "ä»»åŠ¡4", "ä»»åŠ¡5"]
    results = await processor.parallel_retrieve(tasks)

    print(f"å¹¶è¡Œå¤„ç†å®Œæˆ,è€—æ—¶: {time.time() - start:.2f}ç§’")

# è¿è¡Œ
asyncio.run(main())
```

**å¹¶è¡ŒåŠ é€Ÿæ•ˆæœ**:

```
å¤„ç†5ä¸ªæ£€ç´¢ä»»åŠ¡:
â”œâ”€â”€ ä¸²è¡Œæ‰§è¡Œ: 15ç§’
â”œâ”€â”€ å¹¶è¡Œæ‰§è¡Œ(2 workers): 8ç§’ (1.9x)
â”œâ”€â”€ å¹¶è¡Œæ‰§è¡Œ(4 workers): 4ç§’ (3.75x)
â””â”€â”€ å¹¶è¡Œæ‰§è¡Œ(8 workers): 3ç§’ (5x,æ¥è¿‘ä¸Šé™)
```

---

### 10.4 æœªæ¥å‘å±•è·¯çº¿å›¾

```mermaid
gantt
    title SciResearcherå‘å±•è·¯çº¿å›¾
    dateFormat  YYYY-MM
    section MVPé˜¶æ®µ
    åŸºç¡€5-Agentç³»ç»Ÿ           :done, 2024-01, 2024-03
    PDFè§£æ+å‘é‡æ£€ç´¢          :done, 2024-01, 2024-03
    åŸºç¡€é—®ç­”åŠŸèƒ½              :done, 2024-02, 2024-03

    section ä¼˜åŒ–é˜¶æ®µ
    æ··åˆæ£€ç´¢å®ç°              :active, 2024-03, 2024-04
    å›¾è¡¨ç†è§£å¢å¼º              :active, 2024-03, 2024-04
    æ¨ç†åŠ é€Ÿä¼˜åŒ–              :2024-04, 2024-05

    section æ‰©å±•é˜¶æ®µ
    å¤šæ–‡æ¡£ç»¼è¿°                :2024-05, 2024-06
    HF Daily Papersé›†æˆ       :2024-05, 2024-06
    å®éªŒè®¾è®¡å»ºè®®              :2024-06, 2024-07

    section ç”Ÿæ€é˜¶æ®µ
    MCPå·¥å…·å°è£…               :2024-07, 2024-08
    ç¤¾åŒºæ’ä»¶ç³»ç»Ÿ              :2024-08, 2024-09
    äº‘æœåŠ¡éƒ¨ç½²                :2024-09, 2024-10
```

#### ç‰ˆæœ¬è§„åˆ’

**v1.0 (MVP) - 2024 Q1**
- âœ… 5ä¸ªAgentåä½œ
- âœ… PDFå¤šæ¨¡æ€è§£æ
- âœ… åŸºç¡€é—®ç­”åŠŸèƒ½
- âœ… å¼•ç”¨+ç½®ä¿¡åº¦

**v1.5 (ä¼˜åŒ–) - 2024 Q2**
- ğŸ”„ æ··åˆæ£€ç´¢ç­–ç•¥
- ğŸ”„ å›¾è¡¨ç†è§£å¢å¼º
- ğŸ”„ æ¨ç†åŠ é€Ÿ(3x)
- ğŸ”„ æ™ºèƒ½ç¼“å­˜

**v2.0 (æ‰©å±•) - 2024 Q3**
- ğŸ“… å¤šæ–‡æ¡£ç»¼è¿°
- ğŸ“… å®éªŒè®¾è®¡å»ºè®®
- ğŸ“… ç ”ç©¶ç©ºç™½æ¢æµ‹
- ğŸ“… HF Papersé›†æˆ

**v2.5 (ç”Ÿæ€) - 2024 Q4**
- ğŸ“… MCPå·¥å…·å°è£…
- ğŸ“… æ’ä»¶ç³»ç»Ÿ
- ğŸ“… APIæœåŠ¡
- ğŸ“… Webç•Œé¢

**v3.0 (ä¼ä¸š) - 2025 Q1**
- ğŸ“… ç§æœ‰éƒ¨ç½²æ–¹æ¡ˆ
- ğŸ“… ä¼ä¸šçº§åŠŸèƒ½
- ğŸ“… å®šåˆ¶åŒ–æœåŠ¡
- ğŸ“… æŠ€æœ¯æ”¯æŒ

---

### 10.5 ç¤¾åŒºè´¡çŒ®æŒ‡å—

#### è´¡çŒ®æ–¹å‘

```yaml
æ¬¢è¿çš„è´¡çŒ®ç±»å‹:

ä»£ç è´¡çŒ®:
  - æ–°Agentå®ç°
  - æ€§èƒ½ä¼˜åŒ–
  - Bugä¿®å¤
  - æµ‹è¯•ç”¨ä¾‹

æ–‡æ¡£è´¡çŒ®:
  - æ•™ç¨‹ç¼–å†™
  - APIæ–‡æ¡£
  - ä½¿ç”¨æ¡ˆä¾‹
  - ç¿»è¯‘å·¥ä½œ

æ•°æ®è´¡çŒ®:
  - è¯„æµ‹æ•°æ®é›†
  - ç¤ºä¾‹è®ºæ–‡
  - æ ‡æ³¨æ•°æ®

å·¥å…·è´¡çŒ®:
  - MCP Server
  - å¯è§†åŒ–å·¥å…·
  - è¯„ä¼°è„šæœ¬
```

#### å¦‚ä½•è´¡çŒ®

```bash
# 1. Forké¡¹ç›®
git clone https://github.com/your-username/SciResearcher.git

# 2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
git checkout -b feature/new-agent

# 3. å¼€å‘å’Œæµ‹è¯•
# ... ä½ çš„ä»£ç  ...
pytest tests/

# 4. æäº¤
git commit -m "feat: æ·»åŠ æ–°çš„XXX Agent"

# 5. æ¨é€
git push origin feature/new-agent

# 6. åˆ›å»ºPull Request
```

#### ä»£ç è§„èŒƒ

```python
"""
ä»£ç é£æ ¼:
- Python: PEP 8
- ç±»å‹æç¤º: å¿…é¡»
- æ–‡æ¡£å­—ç¬¦ä¸²: Google Style
- æµ‹è¯•è¦†ç›–ç‡: >80%
"""

# ç¤ºä¾‹
from typing import List, Dict, Optional

class NewAgent:
    """æ–°Agentçš„ç®€çŸ­æè¿°

    è¯¦ç»†è¯´æ˜è¿™ä¸ªAgentçš„åŠŸèƒ½å’Œä½¿ç”¨åœºæ™¯ã€‚

    Attributes:
        model: ä½¿ç”¨çš„æ¨¡å‹
        config: é…ç½®å‚æ•°

    Example:
        >>> agent = NewAgent()
        >>> result = agent.process("è¾“å…¥")
    """

    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–Agent

        Args:
            config: å¯é€‰çš„é…ç½®å­—å…¸

        Raises:
            ValueError: å¦‚æœé…ç½®æ— æ•ˆ
        """
        self.config = config or {}

    def process(self, input_text: str) -> Dict[str, Any]:
        """å¤„ç†è¾“å…¥

        Args:
            input_text: è¾“å…¥æ–‡æœ¬

        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸

        Raises:
            ProcessingError: å¦‚æœå¤„ç†å¤±è´¥
        """
        # å®ç°é€»è¾‘
        pass
```

---

è¿™äº›ä¼˜åŒ–å’Œæ‰©å±•æ€è·¯ä¸ºé¡¹ç›®æä¾›äº†æ¸…æ™°çš„æ¼”è¿›è·¯å¾„,ä»MVPåˆ°ä¼ä¸šçº§äº§å“çš„å®Œæ•´è§„åˆ’!

---

## é™„å½•

### A. å®Œæ•´ä¾èµ–åˆ—è¡¨

```txt
# requirements.txt
smolagents>=0.1.0
transformers>=4.37.0
torch>=2.0.0
faiss-cpu>=1.7.4
magic-pdf[full]==0.7.0b1
accelerate>=0.20.0
sentencepiece>=0.1.99
pillow>=9.0.0
pdfplumber>=0.9.0
numpy>=1.24.0
scikit-learn>=1.3.0
```

### B. é¡¹ç›®æ—¶é—´è§„åˆ’

| é˜¶æ®µ | æ—¶é—´ | ä»»åŠ¡ | æ£€æŸ¥ç‚¹ |
|------|------|------|--------|
| **Week 1** | Day 1-2 | ç¯å¢ƒæ­å»º+æ¨¡å‹ä¸‹è½½ | èƒ½è¿è¡Œç¤ºä¾‹ä»£ç  |
| | Day 3-4 | PDFè§£æ+å‘é‡åº“ | èƒ½ç´¢å¼•å’Œæ£€ç´¢ |
| | Day 5-7 | å®ç°5ä¸ªAgent | Agentå•ç‹¬æµ‹è¯•é€šè¿‡ |
| **Week 2** | Day 8-10 | ç³»ç»Ÿé›†æˆ | ç«¯åˆ°ç«¯æµç¨‹è·‘é€š |
| | Day 11-12 | å®éªŒè¯„ä¼° | æ”¶é›†æ€§èƒ½æ•°æ® |
| | Day 13-14 | æ–‡æ¡£ç¼–å†™ | å®ŒæˆæŠ€æœ¯æ–‡æ¡£ |
| **Week 3** | Day 15-17 | ä»£ç ä¼˜åŒ– | ä»£ç è´¨é‡æ£€æŸ¥ |
| | Day 18-19 | è§†é¢‘åˆ¶ä½œ | å½•åˆ¶æ¼”ç¤ºè§†é¢‘ |
| | Day 20-21 | æäº¤å‡†å¤‡ | æ£€æŸ¥æäº¤ææ–™ |

### C. å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# ç¯å¢ƒç®¡ç†
conda create -n sci python=3.10
conda activate sci

# æ¨¡å‹ä¸‹è½½
python scripts/download_models.py

# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# æ„å»ºæ–‡æ¡£
cd docs && make html

# æ‰“åŒ…å‘å¸ƒ
python setup.py sdist bdist_wheel

# ä¸Šä¼ é­”æ­
modelscope upload --repo your-name/sciresearcher
```

---

## æ€»ç»“

è¿™ä»½æ•™ç¨‹æ¶µç›–äº†ä»é›¶å¼€å§‹æ„å»ºSciResearcheré¡¹ç›®çš„å®Œæ•´æµç¨‹:

âœ… **ç†è®ºåŸºç¡€**: Multi-Agentã€RAGã€smolagentsç­‰æ ¸å¿ƒæ¦‚å¿µ
âœ… **å®è·µä»£ç **: 5ä¸ªAgentçš„å®Œæ•´å®ç°
âœ… **ç³»ç»Ÿé›†æˆ**: ç«¯åˆ°ç«¯çš„å·¥ä½œæµç¨‹
âœ… **è¯„ä¼°æ–¹æ³•**: å¦‚ä½•éªŒè¯ç³»ç»Ÿæ€§èƒ½
âœ… **æäº¤æŒ‡å—**: ç¬¦åˆèµ›äº‹è¦æ±‚çš„ææ–™å‡†å¤‡

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. æ­å»ºå¼€å‘ç¯å¢ƒ
2. è·‘é€šç¤ºä¾‹ä»£ç 
3. å‡†å¤‡æµ‹è¯•æ•°æ®
4. å¼€å§‹å®ç°Agent
5. æŒç»­æµ‹è¯•å’Œä¼˜åŒ–

**ç¥ä½ åœ¨AI+ç§‘ç ”åˆ›æ–°èµ›é“å–å¾—å¥½æˆç»©! ğŸ‰**

---

*æœ€åæ›´æ–°: 2024å¹´*
*ä½œè€…: [æ‚¨çš„åå­—]*
*GitHub: [é¡¹ç›®åœ°å€]*
*é­”æ­ç¤¾åŒº: [é¡¹ç›®åœ°å€]*
