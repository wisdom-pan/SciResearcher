"""
å‘é‡æ•°æ®åº“æ¨¡å—
ä½¿ç”¨ChromaDB + é­”æ­Embedding API
"""
import os
import json
from pathlib import Path
from typing import List, Dict, Optional
from openai import OpenAI
import uuid


class VectorDB:
    """åŸºäºChromaDBçš„å‘é‡æ•°æ®åº“"""

    def __init__(self, collection_name: str = "papers"):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            import chromadb
            from chromadb.config import Settings
        except ImportError:
            print("è­¦å‘Š: chromadb æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install chromadb")
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„fallbackå®ç°
            self.collection = None
            self.openai_client = None
            return

        self.client = chromadb.PersistentClient(path="./data/chromadb")
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        self.openai_client = OpenAI(
            api_key=os.getenv("MODELSCOPE_API_KEY"),
            base_url=os.getenv("MODELSCOPE_BASE_URL")
        )

    def embed_text(self, text: str) -> List[float]:
        """ä½¿ç”¨æœ¬åœ°Qwen3-Embedding-0.6Bæ¨¡å‹ç”Ÿæˆæ–‡æœ¬å‘é‡"""
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
        if not hasattr(self, '_local_model'):
            try:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°Embeddingæ¨¡å‹ (Qwen3-Embedding-0.6B)...")
                from sentence_transformers import SentenceTransformer

                # å°è¯•ä»ModelScopeä¸‹è½½æ¨¡å‹
                try:
                    from modelscope import snapshot_download
                    model_dir = snapshot_download('Qwen/Qwen3-Embedding-0.6B')
                    print(f"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}")
                    self._local_model = SentenceTransformer(model_dir)
                except:
                    # å¦‚æœModelScopeå¤±è´¥ï¼Œä½¿ç”¨HuggingFace
                    print("âš ï¸ ModelScopeä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨HuggingFace...")
                    self._local_model = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")

                print("âœ… æœ¬åœ°Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ!")
            except Exception as e:
                print(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨éšæœºå‘é‡: {e}")
                # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                import random
                return [random.random() for _ in range(1024)]  # Qwen3-Embedding-0.6Bçš„ç»´åº¦

        try:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”ŸæˆåµŒå…¥
            embedding = self._local_model.encode(text)
            return embedding.tolist()
        except Exception as e:
            print(f"æœ¬åœ°åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
            import random
            return [random.random() for _ in range(1024)]

    def add_document(self, doc_id: str, content: str, metadata: Dict = None):
        """æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“"""
        if not self.collection:
            print("ChromaDBæœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£")
            return False

        # åˆ†å—
        chunks = self._chunk_text(content)

        if not chunks:
            return False

        # ä¸ºæ¯ä¸ªå—ç”ŸæˆåµŒå…¥ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½ï¼‰
        print(f"æ­£åœ¨ä¸ºæ–‡æ¡£ '{doc_id}' ç”Ÿæˆ {len(chunks)} ä¸ªåµŒå…¥...")
        import time
        embeddings = []

        for i, chunk in enumerate(chunks):
            if i > 0 and i % 10 == 0:
                print(f"  å·²å¤„ç† {i}/{len(chunks)} å—...")
                time.sleep(0.1)  # æ¯10ä¸ªå—æš‚åœ0.1ç§’

            embedding = self.embed_text(chunk)
            embeddings.append(embedding)

        # å‡†å¤‡æ•°æ®
        ids = [f"{doc_id}_{i}" for i in range(len(chunks))]
        metadatas = [
            {**(metadata or {}), "chunk_index": i, "doc_id": doc_id}
            for i in range(len(chunks))
        ]

        # æ·»åŠ åˆ°ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas
        )

        print(f"âœ“ æˆåŠŸæ·»åŠ æ–‡æ¡£ '{doc_id}' ({len(chunks)} ä¸ªå—)")
        return True

    def search(self, query: str, n_results: int = 5) -> List[Dict]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.collection:
            print("ChromaDBæœªåˆå§‹åŒ–ï¼Œæ— æ³•æœç´¢")
            return []

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embed_text(query)

        # æœç´¢
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )):
                formatted_results.append({
                    "content": doc,
                    "metadata": metadata,
                    "score": 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                })

            return formatted_results
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []

    def _chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å—"""
        if not text:
            return []

        chunks = []
        start = 0
        text = text.replace('\n', ' ')

        while start < len(text):
            end = min(start + chunk_size, len(text))

            # å°è¯•åœ¨å¥å·å¤„åˆ†å‰²
            if end < len(text):
                last_period = text.rfind('ã€‚', start, end)
                if last_period > start:
                    end = last_period + 1

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = max(start + chunk_size - overlap, end)

        return chunks

    def list_documents(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        if not self.collection:
            return []

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£ID
            all_data = self.collection.get()
            doc_ids = set()
            for metadata in all_data['metadatas']:
                doc_ids.add(metadata['doc_id'])

            # è·å–æ¯ä¸ªæ–‡æ¡£çš„ä¿¡æ¯
            docs = []
            for doc_id in doc_ids:
                # è·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰å—
                results = self.collection.get(
                    where={"doc_id": doc_id},
                    include=['metadatas']
                )

                if results['metadatas']:
                    docs.append({
                        "doc_id": doc_id,
                        "chunk_count": len(results['metadatas']),
                        "sample_metadata": results['metadatas'][0]
                    })

            return docs
        except Exception as e:
            print(f"åˆ—å‡ºæ–‡æ¡£å¤±è´¥: {e}")
            return []

    def delete_document(self, doc_id: str):
        """åˆ é™¤æ–‡æ¡£"""
        if not self.collection:
            return False

        try:
            # è·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰å—ID
            results = self.collection.get(
                where={"doc_id": doc_id},
                include=['ids']
            )

            if results['ids']:
                self.collection.delete(ids=results['ids'])
                print(f"âœ“ å·²åˆ é™¤æ–‡æ¡£ '{doc_id}'")
                return True

            return False
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def get_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {"error": "ChromaDBæœªåˆå§‹åŒ–"}

        try:
            count = self.collection.count()
            docs = self.list_documents()
            return {
                "total_chunks": count,
                "total_documents": len(docs),
                "documents": docs
            }
        except Exception as e:
            return {"error": str(e)}


# å…¨å±€å‘é‡æ•°æ®åº“å®ä¾‹
vector_db = VectorDB()
