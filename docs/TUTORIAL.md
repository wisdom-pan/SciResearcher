# SciResearcher åˆå­¦è€…æ•™ç¨‹ (MVPç‰ˆæœ¬)

> ğŸ¯ **ç›®æ ‡**: 30åˆ†é’Ÿä»é›¶å¼€å§‹,å­¦ä¼šè¿è¡Œå’Œä½¿ç”¨ç§‘ç ”æ–‡çŒ®åˆ†æç³»ç»Ÿ

---

## ğŸ“š å¿«é€Ÿå¯¼èˆª

- **5åˆ†é’Ÿäº†è§£**: [è¿™ä¸ªç³»ç»Ÿæ˜¯ä»€ä¹ˆ](#è¿™ä¸ªç³»ç»Ÿæ˜¯ä»€ä¹ˆ)
- **10åˆ†é’Ÿå®‰è£…**: [ç¯å¢ƒå‡†å¤‡å’Œå®‰è£…](#ç¯å¢ƒå‡†å¤‡)
- **15åˆ†é’Ÿä¸Šæ‰‹**: [ç¬¬ä¸€æ¬¡ä½¿ç”¨](#ç¬¬ä¸€æ¬¡ä½¿ç”¨)
- **è¿›é˜¶å­¦ä¹ **: [æ·±å…¥ç†è§£](#æ·±å…¥ç†è§£)

---

## è¿™ä¸ªç³»ç»Ÿæ˜¯ä»€ä¹ˆ?

### ä¸€å¥è¯ä»‹ç»

**SciResearcher** = ä¸€ä¸ªä¼š"è¯»è®ºæ–‡"çš„AIåŠ©æ‰‹,å¯ä»¥ç†è§£PDFä¸­çš„æ–‡å­—ã€å›¾è¡¨ã€å…¬å¼,ç„¶åå›ç­”ä½ çš„é—®é¢˜ã€‚

### èƒ½åšä»€ä¹ˆ?

```
ä½ é—®: "è¿™ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆ?"
    â†“
ç³»ç»Ÿ: 1ï¸âƒ£ è§£æPDF (æå–æ–‡å­—+å›¾è¡¨+å…¬å¼)
     2ï¸âƒ£ ç†è§£å›¾è¡¨ (ç”¨AIçœ‹æ‡‚å›¾)
     3ï¸âƒ£ æ£€ç´¢è¯æ® (æ‰¾ç›¸å…³å†…å®¹)
     4ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆ (åŸºäºè¯æ®å›ç­”)
     5ï¸âƒ£ è´¨é‡æ£€æŸ¥ (ç¡®ä¿ç­”æ¡ˆå¯é )
    â†“
è¾“å‡º: "è¿™ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹æ˜¯...
      - åˆ›æ–°ç‚¹1: xxx (è§ç¬¬3é¡µå›¾2)
      - åˆ›æ–°ç‚¹2: xxx (è§è¡¨1)
      ç½®ä¿¡åº¦: 0.85"
```

### ä¸ºä»€ä¹ˆç‰¹åˆ«?

| ä¼ ç»Ÿæ–¹æ³• | SciResearcher |
|---------|---------------|
| âŒ åªèƒ½è¯»æ–‡å­—,çœ‹ä¸æ‡‚å›¾è¡¨ | âœ… æ–‡å­—+å›¾è¡¨+å…¬å¼å…¨éƒ½æ‡‚ |
| âŒ å®¹æ˜“çç¼–ç­”æ¡ˆ | âœ… æœ‰è¯æ®+å¼•ç”¨+ç½®ä¿¡åº¦ |
| âŒ ä¸€ä¸ªAIåšæ‰€æœ‰äº‹ | âœ… 5ä¸ªAIåä½œ(ä¸“ä¸šåˆ†å·¥) |

---

## æ ¸å¿ƒæ¦‚å¿µ (3åˆ†é’Ÿç†è§£)

### 1. ä»€ä¹ˆæ˜¯ Multi-Agent (å¤šæ™ºèƒ½ä½“)?

**å°±åƒä¸€ä¸ªç ”ç©¶å›¢é˜Ÿ**:

```
ä¼ ç»ŸAI = ä¸€ä¸ªäººåšæ‰€æœ‰äº‹æƒ…
    è¯»è®ºæ–‡ â†’ çœ‹å›¾ â†’ å›ç­” â†’ æ£€æŸ¥
    ç»“æœ: æ ·æ ·é€š,æ ·æ ·æ¾

Multi-Agent = 5ä¸ªäººå„å¸å…¶èŒ
    äºº1 (Planner)   â†’ æ‹†è§£ä»»åŠ¡: "éœ€è¦åš3ä»¶äº‹"
    äºº2 (Retriever) â†’ æ£€ç´¢è¯æ®: "æ‰¾åˆ°5æ®µç›¸å…³å†…å®¹"
    äºº3 (Caption)   â†’ ç†è§£å›¾è¡¨: "è¿™å¼ å›¾è¯´æ˜äº†..."
    äºº4 (Reasoner)  â†’ æ¨ç†å›ç­”: "åŸºäºè¯æ®,ç­”æ¡ˆæ˜¯..."
    äºº5 (Reviewer)  â†’ è´¨é‡æ£€æŸ¥: "ç­”æ¡ˆå¯é ,ç½®ä¿¡åº¦0.85"
    ç»“æœ: ä¸“ä¸šåˆ†å·¥,è´¨é‡æ›´é«˜
```

### 2. ä»€ä¹ˆæ˜¯ Vector Database (å‘é‡æ•°æ®åº“)?

**å°±åƒä¸€ä¸ª"è¯­ä¹‰æœç´¢å¼•æ“"**:

```
æ™®é€šæœç´¢ (å…³é”®è¯åŒ¹é…):
    é—®é¢˜: "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?"
    åªèƒ½æ‰¾åˆ°åŒ…å«"æ·±åº¦å­¦ä¹ "è¿™4ä¸ªå­—çš„å†…å®¹

å‘é‡æœç´¢ (è¯­ä¹‰åŒ¹é…):
    é—®é¢˜: "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?"
    èƒ½æ‰¾åˆ°: "ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ ", "AIæ¨¡å‹"
    åŸå› : ç†è§£"æ„æ€ç›¸è¿‘",ä¸åªæ˜¯å­—é¢åŒ¹é…
```

**æˆ‘ä»¬ç”¨çš„æ˜¯ ChromaDB**: è½»é‡ã€ç®€å•ã€è‡ªåŠ¨ä¿å­˜

### 3. ç³»ç»Ÿæ¶æ„ (3å±‚è®¾è®¡)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç”¨æˆ·æé—®                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agents å±‚ (5ä¸ªä¸“é—¨çš„AI)                   â”‚
â”‚   â”œâ”€â”€ Planner   â†’ ä»»åŠ¡åˆ†è§£                  â”‚
â”‚   â”œâ”€â”€ Retriever â†’ è¯æ®æ£€ç´¢                  â”‚
â”‚   â”œâ”€â”€ Caption   â†’ å›¾åƒç†è§£                  â”‚
â”‚   â”œâ”€â”€ Reasoner  â†’ æ¨ç†ç”Ÿæˆ                  â”‚
â”‚   â””â”€â”€ Reviewer  â†’ è´¨é‡æ£€æŸ¥                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tools å±‚ (5ä¸ªå·¥å…·å‡½æ•°)                    â”‚
â”‚   â”œâ”€â”€ parse_pdf      â†’ è§£æPDF             â”‚
â”‚   â”œâ”€â”€ index_documents â†’ å»ºç«‹ç´¢å¼•            â”‚
â”‚   â”œâ”€â”€ search_documents â†’ æœç´¢å†…å®¹           â”‚
â”‚   â”œâ”€â”€ analyze_image   â†’ åˆ†æå›¾è¡¨            â”‚
â”‚   â””â”€â”€ process_paper   â†’ å®Œæ•´æµç¨‹            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Services å±‚ (5ä¸ªæœåŠ¡)                     â”‚
â”‚   â”œâ”€â”€ PDFService      â†’ MinerUäº‘æœåŠ¡        â”‚
â”‚   â”œâ”€â”€ VisionService   â†’ Qwen-VLå›¾åƒç†è§£     â”‚
â”‚   â”œâ”€â”€ EmbeddingService â†’ Qwen3å‘é‡åŒ–       â”‚
â”‚   â”œâ”€â”€ VectorStore     â†’ ChromaDBå­˜å‚¨        â”‚
â”‚   â””â”€â”€ ModelFactory    â†’ APIå®¢æˆ·ç«¯ç®¡ç†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å¤–éƒ¨API (äº‘æœåŠ¡)                          â”‚
â”‚   â”œâ”€â”€ é˜¿é‡Œäº‘ Dashscope (Qwen3æ¨¡å‹)         â”‚
â”‚   â””â”€â”€ MinerUäº‘æœåŠ¡ (PDFè§£æ)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**:
- âœ… **Agentså±‚**: 5ä¸ªAIåä½œ
- âœ… **Toolså±‚**: ç®€å•çš„å·¥å…·å‡½æ•°
- âœ… **Serviceså±‚**: æ ¸å¿ƒåŠŸèƒ½å®ç°
- âœ… **å¤–éƒ¨API**: è°ƒç”¨äº‘æœåŠ¡ (ä¸éœ€è¦æœ¬åœ°éƒ¨ç½²æ¨¡å‹)

---

## ç¯å¢ƒå‡†å¤‡

### ç¬¬ä¸€æ­¥: æ£€æŸ¥Pythonç‰ˆæœ¬

```bash
python --version
# éœ€è¦: Python 3.9 æˆ–æ›´é«˜
```

**å¦‚æœç‰ˆæœ¬å¤ªä½**:
- å» https://www.python.org/downloads/ ä¸‹è½½æœ€æ–°ç‰ˆæœ¬
- æˆ–ä½¿ç”¨ pyenv å®‰è£…: `pyenv install 3.11`

### ç¬¬äºŒæ­¥: è·å–APIå¯†é’¥

#### 2.1 é˜¿é‡Œäº‘ Dashscope APIå¯†é’¥

1. è®¿é—®: https://dashscope.console.aliyun.com/
2. æ³¨å†Œ/ç™»å½•é˜¿é‡Œäº‘è´¦å·
3. ç‚¹å‡»å³ä¸Šè§’å¤´åƒ â†’ "API-KEYç®¡ç†"
4. ç‚¹å‡»"åˆ›å»ºæ–°çš„API-KEY"
5. å¤åˆ¶å¯†é’¥ (æ ¼å¼: `sk-xxxxxxxxxxxxx`)

**å…è´¹é¢åº¦**: æ–°ç”¨æˆ·æœ‰å…è´¹é¢åº¦,å¤Ÿæµ‹è¯•ä½¿ç”¨

#### 2.2 MinerU API Token

1. è®¿é—®: https://mineru.net/
2. æ³¨å†Œ/ç™»å½•è´¦å·
3. è¿›å…¥"APIç®¡ç†"
4. åˆ›å»ºAPI Token
5. å¤åˆ¶token (æ ¼å¼: `sk-xxxxxxxxxxxxx`)

**å…è´¹é¢åº¦**: æ–°ç”¨æˆ·æœ‰å…è´¹è§£ææ¬¡æ•°

### ç¬¬ä¸‰æ­¥: å®‰è£…é¡¹ç›®

```bash
# 1. ä¸‹è½½é¡¹ç›®
git clone <repository-url>
cd SciResearcher

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv venv

# 3. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Mac/Linux:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 4. å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**å®‰è£…æ—¶é—´**: å¤§çº¦3-5åˆ†é’Ÿ

### ç¬¬å››æ­¥: é…ç½®ç¯å¢ƒå˜é‡

```bash
# 1. å¤åˆ¶ç¤ºä¾‹æ–‡ä»¶
cp .env.example .env

# 2. ç¼–è¾‘ .env æ–‡ä»¶
nano .env  # æˆ–ä½¿ç”¨å…¶ä»–ç¼–è¾‘å™¨
```

**å¡«å…¥ä½ çš„APIå¯†é’¥**:

```bash
# é˜¿é‡Œäº‘ Dashscope APIå¯†é’¥
DASHSCOPE_API_KEY=sk-ä½ çš„Dashscopeå¯†é’¥

# MinerU API Token
MINERU_API_TOKEN=sk-ä½ çš„MinerU Token
```

**âš ï¸ æ³¨æ„**:
- ä¸è¦æœ‰å¼•å·
- ä¸è¦æœ‰ç©ºæ ¼
- ä¿å­˜åå…³é—­ç¼–è¾‘å™¨

---

## ç¬¬ä¸€æ¬¡ä½¿ç”¨

### ç¤ºä¾‹1: è§£æä¸€ç¯‡è®ºæ–‡ (æœ€ç®€å•)

```python
from tools.research_tools import process_research_paper
import json

# 1. å‡†å¤‡ä¸€ä¸ªå…¬å¼€çš„PDFé“¾æ¥ (å¿…é¡»æ˜¯å¯ç›´æ¥è®¿é—®çš„URL)
pdf_url = "https://arxiv.org/pdf/1706.03762.pdf"  # Transformerè®ºæ–‡

# 2. è§£æè®ºæ–‡
result = process_research_paper(pdf_url)

# 3. æŸ¥çœ‹ç»“æœ
data = json.loads(result)
print(f"âœ… è§£æå®Œæˆ!")
print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {data['text_length']} å­—ç¬¦")
print(f"ğŸ“Š è¡¨æ ¼æ•°é‡: {data['tables_count']} ä¸ª")
print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {data['images_count']} ä¸ª")
```

**è¿è¡Œ**:
```bash
python your_script.py
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ“¤ æäº¤ PDF: https://arxiv.org/pdf/1706.03762.pdf...
âœ… ä»»åŠ¡ ID: task_12345
ğŸ“„ è¿›åº¦: 5/15 é¡µ
ğŸ“„ è¿›åº¦: 10/15 é¡µ
ğŸ“„ è¿›åº¦: 15/15 é¡µ
ğŸ“¥ ä¸‹è½½ç»“æœ...
ğŸ“Š å‘é‡åŒ– 120 ä¸ªæ–‡æœ¬å—...
âœ… æˆåŠŸæ·»åŠ  120 ä¸ªå‘é‡
ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: 120 ä¸ªå‘é‡
âœ… è§£æå®Œæˆ!
ğŸ“ æ–‡æœ¬é•¿åº¦: 45823 å­—ç¬¦
ğŸ“Š è¡¨æ ¼æ•°é‡: 3 ä¸ª
ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: 8 ä¸ª
```

### ç¤ºä¾‹2: æœç´¢è®ºæ–‡å†…å®¹

```python
from tools.research_tools import search_documents
import json

# æœç´¢é—®é¢˜
query = "ä»€ä¹ˆæ˜¯self-attentionæœºåˆ¶?"

# æ‰§è¡Œæœç´¢
results = search_documents(query, top_k=3)
data = json.loads(results)

# æ˜¾ç¤ºç»“æœ
for i, result in enumerate(data, 1):
    print(f"\nğŸ“Œ ç»“æœ {i}:")
    print(f"ç›¸å…³åº¦åˆ†æ•°: {result['score']:.2f}")
    print(f"å†…å®¹æ‘˜è¦: {result['text'][:150]}...")
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ“Œ ç»“æœ 1:
ç›¸å…³åº¦åˆ†æ•°: 0.23
å†…å®¹æ‘˜è¦: Self-attention, sometimes called intra-attention is an
attention mechanism relating different positions of a single...

ğŸ“Œ ç»“æœ 2:
ç›¸å…³åº¦åˆ†æ•°: 0.31
å†…å®¹æ‘˜è¦: The Transformer is the first transduction model relying
entirely on self-attention to compute representations...
```

### ç¤ºä¾‹3: ä½¿ç”¨Agentå®Œæ•´æµç¨‹

**åˆ›å»ºæ–‡ä»¶**: `test_agent.py`

```python
from agents.research_agents import (
    PlannerAgent,
    RetrieverAgent,
    ReasonerAgent,
    ReviewerAgent
)

# ç ”ç©¶é—®é¢˜
question = "Transformeræ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆ?"

print("ğŸ¤” ç ”ç©¶é—®é¢˜:", question)
print("\n" + "="*50)

# 1ï¸âƒ£ ä»»åŠ¡åˆ†è§£
print("\n1ï¸âƒ£ ä»»åŠ¡åˆ†è§£ä¸­...")
planner = PlannerAgent()
plan = planner.plan(question)
print(f"ğŸ“‹ åˆ†è§£ä¸º {len(plan['sub_tasks'])} ä¸ªå­ä»»åŠ¡:")
for i, task in enumerate(plan['sub_tasks'], 1):
    print(f"   {i}. {task}")

# 2ï¸âƒ£ è¯æ®æ£€ç´¢
print("\n2ï¸âƒ£ æ£€ç´¢è¯æ®ä¸­...")
retriever = RetrieverAgent()
evidence = retriever.retrieve(plan['sub_tasks'], top_k=5)
total_evidence = sum(e['evidence_count'] for e in evidence)
print(f"ğŸ” æ‰¾åˆ° {total_evidence} æ¡è¯æ®")

# 3ï¸âƒ£ æ¨ç†ç”Ÿæˆ
print("\n3ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆä¸­...")
reasoner = ReasonerAgent()
answer = reasoner.reason(
    question=question,
    evidence=evidence,
    require_citations=True
)
print(f"ğŸ’¡ ç­”æ¡ˆ: {answer['answer']}")
print(f"ğŸ“Š ç½®ä¿¡åº¦: {answer['confidence']:.2f}")

# 4ï¸âƒ£ è´¨é‡æ£€æŸ¥
print("\n4ï¸âƒ£ è´¨é‡æ£€æŸ¥ä¸­...")
reviewer = ReviewerAgent()
review = reviewer.review(
    question=question,
    answer=answer['answer'],
    evidence=evidence,
    confidence=answer['confidence']
)
print(f"âœ… æœ€ç»ˆç½®ä¿¡åº¦: {review['final_confidence']:.2f}")
print(f"ğŸ”„ éœ€è¦è¿­ä»£: {'æ˜¯' if review['need_iterate'] else 'å¦'}")
if review['issues']:
    print(f"âš ï¸ å‘ç°é—®é¢˜: {', '.join(review['issues'])}")

print("\n" + "="*50)
print("ğŸ‰ åˆ†æå®Œæˆ!")
```

**è¿è¡Œ**:
```bash
python test_agent.py
```

**é¢„æœŸè¾“å‡º**:
```
ğŸ¤” ç ”ç©¶é—®é¢˜: Transformeræ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆ?

==================================================

1ï¸âƒ£ ä»»åŠ¡åˆ†è§£ä¸­...
ğŸ“‹ åˆ†è§£ä¸º 3 ä¸ªå­ä»»åŠ¡:
   1. ç†è§£Transformerçš„æ¶æ„è®¾è®¡
   2. åˆ†æself-attentionæœºåˆ¶
   3. å¯¹æ¯”ä¼ ç»ŸRNN/LSTMæ¨¡å‹

2ï¸âƒ£ æ£€ç´¢è¯æ®ä¸­...
ğŸ” æ‰¾åˆ° 15 æ¡è¯æ®

3ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆä¸­...
ğŸ’¡ ç­”æ¡ˆ: Transformeræ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°ä¸»è¦åŒ…æ‹¬:
1. å®Œå…¨åŸºäºattentionæœºåˆ¶,æ‘’å¼ƒäº†å¾ªç¯ç»“æ„
2. å¼•å…¥multi-head self-attention
3. ä½ç½®ç¼–ç (Positional Encoding)è®¾è®¡
...
ğŸ“Š ç½®ä¿¡åº¦: 0.87

4ï¸âƒ£ è´¨é‡æ£€æŸ¥ä¸­...
âœ… æœ€ç»ˆç½®ä¿¡åº¦: 0.85
ğŸ”„ éœ€è¦è¿­ä»£: å¦

==================================================
ğŸ‰ åˆ†æå®Œæˆ!
```

---

## æ·±å…¥ç†è§£

### æ–‡ä»¶ç»“æ„è§£æ

```
SciResearcher/
â”œâ”€â”€ services/              # æ ¸å¿ƒæœåŠ¡ (5ä¸ªæ–‡ä»¶)
â”‚   â”œâ”€â”€ model_factory.py   # APIå®¢æˆ·ç«¯ç®¡ç†
â”‚   â”œâ”€â”€ pdf_service.py     # PDFè§£æ (MinerU)
â”‚   â”œâ”€â”€ vision_service.py  # å›¾åƒç†è§£ (Qwen-VL)
â”‚   â”œâ”€â”€ embedding_service.py # æ–‡æœ¬å‘é‡åŒ– (Qwen3)
â”‚   â””â”€â”€ vector_store.py    # å‘é‡å­˜å‚¨ (ChromaDB)
â”‚
â”œâ”€â”€ agents/                # AIæ™ºèƒ½ä½“ (1ä¸ªæ–‡ä»¶)
â”‚   â””â”€â”€ research_agents.py # 5ä¸ªAgentç±»
â”‚
â”œâ”€â”€ tools/                 # å·¥å…·å‡½æ•° (1ä¸ªæ–‡ä»¶)
â”‚   â””â”€â”€ research_tools.py  # 5ä¸ª@toolå‡½æ•°
â”‚
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½• (è‡ªåŠ¨åˆ›å»º)
â”‚   â””â”€â”€ vector_index/      # ChromaDBæ•°æ®
â”‚
â”œâ”€â”€ main.py                # ä¸»ç¨‹åº (å¯é€‰)
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .env.example           # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ .env                   # ä½ çš„é…ç½® (ä¸è¦ä¸Šä¼ !)
â””â”€â”€ TUTORIAL.md            # æœ¬æ•™ç¨‹
```

### æ¯ä¸ªæ¨¡å—çš„ä½œç”¨

#### 1. services/model_factory.py (25è¡Œ)

**ä½œç”¨**: åˆ›å»ºå’Œç®¡ç†APIå®¢æˆ·ç«¯

```python
# æ ¸å¿ƒä»£ç 
class ModelFactory:
    @classmethod
    def get_client(cls, provider="dashscope"):
        return OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
```

**ä¸ºä»€ä¹ˆéœ€è¦**: ç»Ÿä¸€ç®¡ç†APIå®¢æˆ·ç«¯,é¿å…é‡å¤åˆ›å»º

#### 2. services/pdf_service.py (125è¡Œ)

**ä½œç”¨**: è°ƒç”¨MinerUäº‘æœåŠ¡è§£æPDF

**æ ¸å¿ƒæµç¨‹**:
```
æäº¤PDF URL â†’ è½®è¯¢è§£æçŠ¶æ€ â†’ ä¸‹è½½ZIPç»“æœ â†’ æå–markdown/è¡¨æ ¼/å›¾ç‰‡
```

#### 3. services/vision_service.py (32è¡Œ)

**ä½œç”¨**: ç”¨Qwen-VLç†è§£å›¾åƒ

**ä½¿ç”¨åœºæ™¯**: åˆ†æè®ºæ–‡ä¸­çš„å›¾è¡¨ã€å…¬å¼

#### 4. services/embedding_service.py (22è¡Œ)

**ä½œç”¨**: æŠŠæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

**ä¸ºä»€ä¹ˆéœ€è¦**: å‘é‡æ•°æ®åº“éœ€è¦æ•°å­—å‘é‡,ä¸èƒ½ç›´æ¥å­˜æ–‡æœ¬

#### 5. services/vector_store.py (115è¡Œ)

**ä½œç”¨**: å­˜å‚¨å’Œæœç´¢å‘é‡

**æ ¸å¿ƒåŠŸèƒ½**:
- `add_texts()`: æ·»åŠ æ–‡æ¡£
- `search()`: è¯­ä¹‰æœç´¢
- `save()`: ä¿å­˜ (è‡ªåŠ¨)

### æ•°æ®æµåŠ¨è¿‡ç¨‹

```
ç”¨æˆ·æé—®: "Transformerçš„åˆ›æ–°ç‚¹?"
    â†“
ã€1. ä»»åŠ¡åˆ†è§£ã€‘PlannerAgent
    è¾“å…¥: "Transformerçš„åˆ›æ–°ç‚¹?"
    è¾“å‡º: ["ç†è§£æ¶æ„", "åˆ†æattention", "å¯¹æ¯”RNN"]
    â†“
ã€2. è¯æ®æ£€ç´¢ã€‘RetrieverAgent
    è¾“å…¥: ["ç†è§£æ¶æ„", "åˆ†æattention", "å¯¹æ¯”RNN"]
    è¿‡ç¨‹:
        - embedding_service.embed("ç†è§£æ¶æ„") â†’ å‘é‡
        - vector_store.search(å‘é‡) â†’ ç›¸å…³æ–‡æœ¬
    è¾“å‡º: 15æ¡è¯æ®
    â†“
ã€3. å›¾åƒç†è§£ã€‘CaptionAgent (å¦‚æœæœ‰å›¾)
    è¾“å…¥: "figure_1.jpg"
    è¿‡ç¨‹:
        - vision_service.analyze(image, question)
    è¾“å‡º: "è¿™å¼ å›¾å±•ç¤ºäº†attentionæœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹..."
    â†“
ã€4. æ¨ç†ç”Ÿæˆã€‘ReasonerAgent
    è¾“å…¥: é—®é¢˜ + è¯æ® + å›¾åƒæè¿°
    è¿‡ç¨‹:
        - æ„å»ºprompt
        - è°ƒç”¨Qwen3æ¨¡å‹
        - è§£æJSONç»“æœ
    è¾“å‡º: {"answer": "...", "confidence": 0.87}
    â†“
ã€5. è´¨é‡æ£€æŸ¥ã€‘ReviewerAgent
    è¾“å…¥: é—®é¢˜ + ç­”æ¡ˆ + è¯æ® + ç½®ä¿¡åº¦
    è¿‡ç¨‹:
        - è§„åˆ™æ£€æŸ¥ (é•¿åº¦ã€ç½®ä¿¡åº¦ã€è¯æ®æ•°é‡)
        - LLMæ£€æŸ¥ (å®Œæ•´æ€§ã€é€»è¾‘æ€§)
    è¾“å‡º: {"final_confidence": 0.85, "need_iterate": false}
    â†“
è¿”å›ç»™ç”¨æˆ·
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦ç”¨äº‘API,ä¸èƒ½æœ¬åœ°éƒ¨ç½²å—?

**ç­”**: å¯ä»¥,ä½†ä¸æ¨èåˆå­¦è€…è¿™æ ·åš

**åŸå› **:
- âŒ Qwen3-72Béœ€è¦4å¼ A100 GPU (æˆæœ¬>10ä¸‡)
- âŒ ç¯å¢ƒé…ç½®å¤æ‚ (CUDAã€PyTorchã€æ¨¡å‹ä¸‹è½½)
- âœ… äº‘APIç®€å•ã€ä¾¿å®œã€ç¨³å®š

**æˆæœ¬å¯¹æ¯”**:
- æœ¬åœ°éƒ¨ç½²: >10ä¸‡å…ƒç¡¬ä»¶ + é«˜ç”µè´¹
- äº‘API: å…è´¹é¢åº¦å¤Ÿæµ‹è¯•,ä»˜è´¹ä¹Ÿå¾ˆä¾¿å®œ (0.001å…ƒ/åƒtoken)

### Q2: PDFå¿…é¡»æ˜¯å…¬å¼€URLå—?

**ç­”**: æ˜¯çš„,MinerUäº‘æœåŠ¡éœ€è¦èƒ½è®¿é—®çš„URL

**è§£å†³æ–¹æ¡ˆ**:
1. ä¸Šä¼ åˆ°GitHub Release
2. ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ‰˜ç®¡ (å¦‚transfer.sh)
3. è‡ªå·±æ­å»ºç®€å•çš„æ–‡ä»¶æœåŠ¡å™¨

### Q3: ChromaDBæ•°æ®å­˜åœ¨å“ªé‡Œ?

**ç­”**: `./data/vector_index/` ç›®å½•

**æŸ¥çœ‹æ•°æ®é‡**:
```python
from tools.research_tools import vector_service
print(f"å½“å‰å‘é‡æ•°: {vector_service.collection.count()}")
```

**æ¸…ç©ºæ•°æ®**:
```python
vector_service.reset()
```

### Q4: å¦‚ä½•è°ƒè¯•é”™è¯¯?

**æ–¹æ³•1: æ‰“å°å˜é‡**
```python
print(f"Debug: plan = {plan}")
```

**æ–¹æ³•2: ä½¿ç”¨try-except**
```python
try:
    result = process_research_paper(pdf_url)
except Exception as e:
    print(f"é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
```

**æ–¹æ³•3: æ£€æŸ¥APIå¯†é’¥**
```python
import os
print(f"APIå¯†é’¥å­˜åœ¨: {bool(os.getenv('DASHSCOPE_API_KEY'))}")
```

### Q5: ç½®ä¿¡åº¦æ˜¯ä»€ä¹ˆæ„æ€?

**ç­”**: 0-1ä¹‹é—´çš„æ•°å­—,è¡¨ç¤ºAIå¯¹ç­”æ¡ˆçš„"ç¡®ä¿¡ç¨‹åº¦"

| ç½®ä¿¡åº¦ | å«ä¹‰ | å¤„ç†å»ºè®® |
|-------|------|---------|
| 0.8-1.0 | éå¸¸ç¡®å®š | å¯ä»¥ç›´æ¥ä½¿ç”¨ |
| 0.6-0.8 | æ¯”è¾ƒç¡®å®š | å»ºè®®éªŒè¯ |
| <0.6 | ä¸å¤ªç¡®å®š | éœ€è¦äººå·¥æ£€æŸ¥ |

**è®¡ç®—ä¾æ®**:
- è¯æ®æ•°é‡ (è¶Šå¤šè¶Šå¥½)
- è¯æ®ç›¸å…³æ€§ (è¶Šç›¸å…³è¶Šå¥½)
- LLMè‡ªæˆ‘è¯„ä¼°

---

## å­¦ä¹ è·¯å¾„

### ç¬¬1å¤©: ç¯å¢ƒå‡†å¤‡å’Œç¬¬ä¸€æ¬¡è¿è¡Œ
- âœ… å®‰è£…Pythonå’Œä¾èµ–
- âœ… è·å–APIå¯†é’¥
- âœ… è¿è¡Œç¤ºä¾‹1 (è§£æPDF)
- âœ… è¿è¡Œç¤ºä¾‹2 (æœç´¢)

### ç¬¬2-3å¤©: ç†è§£æ ¸å¿ƒæ¦‚å¿µ
- âœ… é˜…è¯»"æ ¸å¿ƒæ¦‚å¿µ"ç« èŠ‚
- âœ… ç†è§£Multi-Agentæ¶æ„
- âœ… ç†è§£å‘é‡æ•°æ®åº“åŸç†
- âœ… è¿è¡Œç¤ºä¾‹3 (å®Œæ•´æµç¨‹)

### ç¬¬4-5å¤©: æ·±å…¥ä»£ç 
- âœ… é˜…è¯»services/ä»£ç 
- âœ… ç†è§£æ¯ä¸ªAgentçš„ä½œç”¨
- âœ… ä¿®æ”¹å‚æ•°è¯•è¯• (top_k, temperature)

### ç¬¬6-7å¤©: è‡ªå·±å†™ä»£ç 
- âœ… å†™ä¸€ä¸ªç®€å•çš„æŸ¥è¯¢è„šæœ¬
- âœ… æ‰¹é‡å¤„ç†å¤šç¯‡è®ºæ–‡
- âœ… è‡ªå®šä¹‰Agentå‚æ•°

---

## è¿›é˜¶æ€è·¯å’Œæ‰©å±•æ–¹å‘

å­¦å®ŒåŸºç¡€æ•™ç¨‹å,ä½ å¯ä»¥å°è¯•ä»¥ä¸‹è¿›é˜¶åŠŸèƒ½å’Œæ‰©å±•æ–¹å‘ã€‚

### ğŸš€ è¿›é˜¶åŠŸèƒ½ (æå‡ç³»ç»Ÿèƒ½åŠ›)

#### 1. æ‰¹é‡å¤„ç†å¤šç¯‡è®ºæ–‡

**åœºæ™¯**: åˆ†ææŸä¸ªç ”ç©¶æ–¹å‘çš„10ç¯‡ç›¸å…³è®ºæ–‡

```python
from tools.research_tools import process_research_paper
import json

# è®ºæ–‡åˆ—è¡¨
papers = [
    "https://arxiv.org/pdf/1706.03762.pdf",  # Transformer
    "https://arxiv.org/pdf/1810.04805.pdf",  # BERT
    "https://arxiv.org/pdf/2005.14165.pdf",  # GPT-3
    # ... æ›´å¤šè®ºæ–‡
]

# æ‰¹é‡å¤„ç†
results = []
for i, url in enumerate(papers, 1):
    print(f"\nğŸ“„ å¤„ç†è®ºæ–‡ {i}/{len(papers)}")
    try:
        result = process_research_paper(url)
        results.append(json.loads(result))
        print(f"âœ… æˆåŠŸ: {url}")
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")

# ç»Ÿè®¡åˆ†æ
total_text = sum(r['text_length'] for r in results)
total_tables = sum(r['tables_count'] for r in results)
total_images = sum(r['images_count'] for r in results)

print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
print(f"æ€»æ–‡æœ¬é‡: {total_text:,} å­—ç¬¦")
print(f"æ€»è¡¨æ ¼æ•°: {total_tables} ä¸ª")
print(f"æ€»å›¾ç‰‡æ•°: {total_images} ä¸ª")
```

**æ”¹è¿›ç‚¹**:
- âœ… æ·»åŠ è¿›åº¦æ¡ (ä½¿ç”¨tqdm)
- âœ… å¤±è´¥é‡è¯•æœºåˆ¶
- âœ… ä¿å­˜ä¸­é—´ç»“æœ

#### 2. æ–‡çŒ®ç»¼è¿°ç”Ÿæˆ

**åœºæ™¯**: åŸºäºå¤šç¯‡è®ºæ–‡ç”Ÿæˆç ”ç©¶ç»¼è¿°

```python
from agents.research_agents import ReasonerAgent
from tools.research_tools import search_documents

# ç»¼è¿°ä¸»é¢˜
topic = "Transformeræ¶æ„åœ¨NLPä¸­çš„åº”ç”¨"

# 1. æ£€ç´¢ç›¸å…³å†…å®¹
print("ğŸ” æ£€ç´¢ç›¸å…³æ–‡çŒ®...")
evidence_queries = [
    "TransformeråŸºæœ¬åŸç†",
    "Transformeråœ¨æœºå™¨ç¿»è¯‘ä¸­çš„åº”ç”¨",
    "Transformeråœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„åº”ç”¨",
    "Transformerçš„æ”¹è¿›å˜ä½“"
]

all_evidence = []
for query in evidence_queries:
    results = search_documents(query, top_k=10)
    all_evidence.append({
        "task": query,
        "evidence": json.loads(results)
    })

# 2. ç”Ÿæˆç»¼è¿°
print("\nğŸ“ ç”Ÿæˆæ–‡çŒ®ç»¼è¿°...")
reasoner = ReasonerAgent()
review = reasoner.reason(
    question=f"è¯·ç”Ÿæˆå…³äº'{topic}'çš„æ–‡çŒ®ç»¼è¿°",
    evidence=all_evidence,
    require_citations=True
)

print(f"\nğŸ“„ ç»¼è¿°å†…å®¹:\n{review['answer']}")
print(f"\nğŸ“Š ç½®ä¿¡åº¦: {review['confidence']:.2f}")
```

**æ‰©å±•æ–¹å‘**:
- ğŸ“Š æ·»åŠ å¯è§†åŒ– (æ—¶é—´çº¿ã€å¼•ç”¨ç½‘ç»œ)
- ğŸ“ å¯¼å‡ºä¸ºMarkdown/PDF
- ğŸ”„ è¿­ä»£ä¼˜åŒ– (åŸºäºReviewerAgentåé¦ˆ)

#### 3. å›¾è¡¨æ·±åº¦åˆ†æ

**åœºæ™¯**: æå–è®ºæ–‡ä¸­æ‰€æœ‰å›¾è¡¨å¹¶åˆ†æ

```python
from services import PDFService, VisionService
import zipfile
import io

# è§£æPDFå¹¶è·å–å›¾åƒ
pdf_service = PDFService()
result = pdf_service.parse("https://arxiv.org/pdf/1706.03762.pdf")

# åˆ†ææ¯å¼ å›¾
vision_service = VisionService()
for i, image_info in enumerate(result['images'], 1):
    print(f"\nğŸ–¼ï¸ åˆ†æå›¾ç‰‡ {i}/{len(result['images'])}")

    # å‡è®¾å›¾ç‰‡å·²ä¸‹è½½åˆ°æœ¬åœ°
    image_path = f"./data/images/{image_info['path_in_zip']}"

    # å¤šè§’åº¦åˆ†æ
    questions = [
        "è¿™å¼ å›¾å±•ç¤ºäº†ä»€ä¹ˆå†…å®¹?",
        "å›¾ä¸­çš„å…³é”®æ•°æ®æ˜¯ä»€ä¹ˆ?",
        "è¿™å¼ å›¾æ”¯æŒä»€ä¹ˆç»“è®º?"
    ]

    for q in questions:
        answer = vision_service.analyze(image_path, q)
        print(f"  Q: {q}")
        print(f"  A: {answer}\n")
```

**æ”¹è¿›æ–¹å‘**:
- ğŸ“Š å›¾è¡¨ç±»å‹è¯†åˆ« (æŠ˜çº¿å›¾/æŸ±çŠ¶å›¾/æµç¨‹å›¾)
- ğŸ”¢ æ•°æ®æå– (OCRæ•°å€¼)
- ğŸ“ˆ è¶‹åŠ¿åˆ†æ

#### 4. è‡ªå®šä¹‰Agentå¼€å‘

**åœºæ™¯**: åˆ›å»ºä¸€ä¸ª"ç›¸å…³å·¥ä½œæ¨è"Agent

```python
from services import ModelFactory
from tools.research_tools import vector_service

class RelatedWorkAgent:
    """ç›¸å…³å·¥ä½œæ¨èAgent"""

    def __init__(self, model_name="qwen-plus"):
        self.client = ModelFactory.get_client()
        self.model_name = model_name

    def recommend(self, current_paper_summary: str, top_k: int = 5):
        """æ¨èç›¸å…³è®ºæ–‡

        Args:
            current_paper_summary: å½“å‰è®ºæ–‡æ‘˜è¦
            top_k: æ¨èæ•°é‡
        """
        # 1. æå–å…³é”®æ¦‚å¿µ
        prompt = f"""ä»ä»¥ä¸‹è®ºæ–‡æ‘˜è¦ä¸­æå–3-5ä¸ªæ ¸å¿ƒç ”ç©¶ä¸»é¢˜:

æ‘˜è¦: {current_paper_summary}

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{"keywords": ["ä¸»é¢˜1", "ä¸»é¢˜2", "ä¸»é¢˜3"]}}
"""
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}]
        )

        keywords_text = response.choices[0].message.content
        keywords = json.loads(keywords_text)['keywords']

        # 2. æ£€ç´¢ç›¸å…³æ–‡çŒ®
        recommendations = []
        for keyword in keywords:
            results = vector_service.search(keyword, top_k=3)
            recommendations.extend(results)

        # 3. å»é‡æ’åº
        seen = set()
        unique_recs = []
        for rec in recommendations:
            text_hash = hash(rec['text'][:100])
            if text_hash not in seen:
                seen.add(text_hash)
                unique_recs.append(rec)

        return sorted(unique_recs, key=lambda x: x['score'])[:top_k]

# ä½¿ç”¨ç¤ºä¾‹
agent = RelatedWorkAgent()
summary = "æœ¬æ–‡æå‡ºäº†Transformeræ¶æ„..."
recs = agent.recommend(summary, top_k=5)

print("ğŸ“š ç›¸å…³å·¥ä½œæ¨è:")
for i, rec in enumerate(recs, 1):
    print(f"{i}. {rec['text'][:200]}... (ç›¸å…³åº¦: {rec['score']:.2f})")
```

### ğŸŒŸ æ‰©å±•æ–¹å‘ (åˆ›æ–°åŠŸèƒ½)

#### æ‰©å±•1: ç ”ç©¶ç©ºç™½æ¢æµ‹

**æ¦‚å¿µ**: åˆ†æå½“å‰ç ”ç©¶æ–¹å‘çš„æœªè§£å†³é—®é¢˜

**å®ç°æ€è·¯**:
```python
class ResearchGapAgent:
    """ç ”ç©¶ç©ºç™½æ¢æµ‹Agent"""

    def detect_gaps(self, papers: list, topic: str):
        """
        åˆ†æç­–ç•¥:
        1. æå–æ‰€æœ‰è®ºæ–‡çš„å±€é™æ€§éƒ¨åˆ†
        2. æå–æ‰€æœ‰"Future Work"éƒ¨åˆ†
        3. åˆ†ææœªè¢«å……åˆ†ç ”ç©¶çš„å­é¢†åŸŸ
        4. ç”Ÿæˆç ”ç©¶æœºä¼šæŠ¥å‘Š
        """
        pass
```

**åº”ç”¨åœºæ™¯**:
- ğŸ“– æ–‡çŒ®ç»¼è¿°æ’°å†™
- ğŸ’¡ ç ”ç©¶é€‰é¢˜
- ğŸ¯ ç¡®å®šç ”ç©¶æ–¹å‘

#### æ‰©å±•2: å®éªŒæ–¹æ³•å¯¹æ¯”

**æ¦‚å¿µ**: è‡ªåŠ¨æå–å’Œå¯¹æ¯”ä¸åŒè®ºæ–‡çš„å®éªŒè®¾ç½®

**å®ç°æ€è·¯**:
```python
class ExperimentComparisonAgent:
    """å®éªŒæ–¹æ³•å¯¹æ¯”Agent"""

    def compare_experiments(self, papers: list):
        """
        å¯¹æ¯”ç»´åº¦:
        1. æ•°æ®é›† (åç§°ã€è§„æ¨¡ã€æ¥æº)
        2. è¯„ä¼°æŒ‡æ ‡ (Accuracyã€F1ã€BLEUç­‰)
        3. åŸºçº¿æ¨¡å‹
        4. å®éªŒç¯å¢ƒ (ç¡¬ä»¶ã€è¶…å‚æ•°)

        è¾“å‡º:
        - å¯¹æ¯”è¡¨æ ¼
        - æ€§èƒ½è¶‹åŠ¿å›¾
        - æ–¹æ³•æ¼”è¿›åˆ†æ
        """
        pass
```

**åº”ç”¨åœºæ™¯**:
- ğŸ“Š å®éªŒè®¾è®¡å‚è€ƒ
- ğŸ† SOTAç»“æœè¿½è¸ª
- ğŸ“ˆ æ€§èƒ½åŸºå‡†å»ºç«‹

#### æ‰©å±•3: å¼•ç”¨ç½‘ç»œåˆ†æ

**æ¦‚å¿µ**: æ„å»ºè®ºæ–‡å¼•ç”¨å…³ç³»å›¾è°±

**å®ç°æ€è·¯**:
```python
class CitationNetworkAgent:
    """å¼•ç”¨ç½‘ç»œåˆ†æAgent"""

    def build_network(self, seed_paper: str, depth: int = 2):
        """
        æ„å»ºæ­¥éª¤:
        1. æå–ç§å­è®ºæ–‡çš„å¼•ç”¨åˆ—è¡¨
        2. é€’å½’è·å–è¢«å¼•è®ºæ–‡
        3. æ„å»ºå¼•ç”¨å›¾è°±
        4. è®¡ç®—é‡è¦æ€§æŒ‡æ ‡ (PageRankã€ä¸­å¿ƒæ€§)

        å¯è§†åŒ–:
        - å¼•ç”¨å…³ç³»å›¾
        - ç ”ç©¶æ¼”è¿›æ—¶é—´çº¿
        - æ ¸å¿ƒè®ºæ–‡è¯†åˆ«
        """
        pass
```

**æŠ€æœ¯æ ˆ**:
- NetworkX (å›¾åˆ†æ)
- Plotly (äº¤äº’å¯è§†åŒ–)
- Semantic Scholar API (å¼•ç”¨æ•°æ®)

#### æ‰©å±•4: å¤šè¯­è¨€æ”¯æŒ

**æ¦‚å¿µ**: æ”¯æŒä¸­è‹±æ–‡æ··åˆåˆ†æ

**å®ç°è¦ç‚¹**:
```python
# 1. æ£€æµ‹è¯­è¨€
from langdetect import detect

def detect_language(text):
    return detect(text)

# 2. æ ¹æ®è¯­è¨€é€‰æ‹©ä¸åŒçš„æç¤ºè¯æ¨¡æ¿
PROMPTS = {
    "zh": "è¯·åŸºäºä»¥ä¸‹è¯æ®å›ç­”é—®é¢˜...",
    "en": "Please answer the question based on the evidence..."
}

# 3. ç¿»è¯‘åŠŸèƒ½ (å¯é€‰)
def translate_if_needed(text, target_lang="zh"):
    # ä½¿ç”¨Qwen3ç¿»è¯‘åŠŸèƒ½
    pass
```

#### æ‰©å±•5: çŸ¥è¯†å›¾è°±æ„å»º

**æ¦‚å¿µ**: ä»è®ºæ–‡ä¸­æå–å®ä½“å’Œå…³ç³»

**å®ç°ç¤ºä¾‹**:
```python
class KnowledgeGraphAgent:
    """çŸ¥è¯†å›¾è°±æ„å»ºAgent"""

    def extract_entities(self, text: str):
        """
        æå–å®ä½“:
        - æ¨¡å‹åç§° (BERT, GPT-3, Transformer)
        - æ•°æ®é›† (ImageNet, COCO, SQuAD)
        - è¯„ä¼°æŒ‡æ ‡ (Accuracy, F1-Score)
        - ç ”ç©¶æœºæ„ (OpenAI, Google, Meta)
        """
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç§‘ç ”å®ä½“:

æ–‡æœ¬: {text}

è¯·è¿”å›JSON:
{{
    "models": [...],
    "datasets": [...],
    "metrics": [...],
    "organizations": [...]
}}
"""
        # è°ƒç”¨LLMæå–
        pass

    def extract_relations(self, text: str):
        """
        æå–å…³ç³»:
        - "æ¨¡å‹A" åœ¨ "æ•°æ®é›†B" ä¸Šè¾¾åˆ° "æŒ‡æ ‡C"
        - "è®ºæ–‡X" æ”¹è¿›äº† "æ¨¡å‹Y"
        - "æ–¹æ³•A" ä¼˜äº "æ–¹æ³•B"
        """
        pass
```

**åº”ç”¨**:
- ğŸ” æ™ºèƒ½é—®ç­”
- ğŸ“Š é¢†åŸŸçŸ¥è¯†å¯è§†åŒ–
- ğŸ”— å®ä½“å…³è”åˆ†æ

### ğŸ› ï¸ å·¥ç¨‹ä¼˜åŒ–æ–¹å‘

#### ä¼˜åŒ–1: ç¼“å­˜æœºåˆ¶

**é—®é¢˜**: é‡å¤è§£æåŒä¸€ç¯‡è®ºæ–‡æµªè´¹æ—¶é—´å’ŒAPIé¢åº¦

**è§£å†³æ–¹æ¡ˆ**:
```python
import hashlib
import pickle
from pathlib import Path

class CacheManager:
    """è§£æç»“æœç¼“å­˜"""

    def __init__(self, cache_dir="./data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_cache_key(self, pdf_url: str) -> str:
        return hashlib.md5(pdf_url.encode()).hexdigest()

    def get(self, pdf_url: str):
        cache_file = self.cache_dir / f"{self.get_cache_key(pdf_url)}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def set(self, pdf_url: str, data):
        cache_file = self.cache_dir / f"{self.get_cache_key(pdf_url)}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)

# ä½¿ç”¨
cache = CacheManager()
pdf_url = "https://arxiv.org/pdf/1706.03762.pdf"

result = cache.get(pdf_url)
if result is None:
    result = pdf_service.parse(pdf_url)
    cache.set(pdf_url, result)
```

#### ä¼˜åŒ–2: å¼‚æ­¥å¤„ç†

**é—®é¢˜**: æ‰¹é‡å¤„ç†è®ºæ–‡æ—¶ä¸²è¡Œå¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_paper_async(pdf_url: str):
    """å¼‚æ­¥å¤„ç†å•ç¯‡è®ºæ–‡"""
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as executor:
        result = await loop.run_in_executor(
            executor,
            process_research_paper,
            pdf_url
        )
    return result

async def batch_process(pdf_urls: list):
    """æ‰¹é‡å¼‚æ­¥å¤„ç†"""
    tasks = [process_paper_async(url) for url in pdf_urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results

# ä½¿ç”¨
urls = ["url1", "url2", "url3"]
results = asyncio.run(batch_process(urls))
```

#### ä¼˜åŒ–3: Webç•Œé¢

**æŠ€æœ¯æ ˆ**: Gradio / Streamlit

```python
import gradio as gr

def research_interface(pdf_url, question):
    """ç®€å•çš„Webç•Œé¢"""
    # 1. è§£æPDF
    result = process_research_paper(pdf_url)

    # 2. å›ç­”é—®é¢˜
    planner = PlannerAgent()
    plan = planner.plan(question)

    retriever = RetrieverAgent()
    evidence = retriever.retrieve(plan['sub_tasks'])

    reasoner = ReasonerAgent()
    answer = reasoner.reason(question, evidence)

    return answer['answer'], answer['confidence']

# åˆ›å»ºç•Œé¢
demo = gr.Interface(
    fn=research_interface,
    inputs=[
        gr.Textbox(label="PDF URL"),
        gr.Textbox(label="ç ”ç©¶é—®é¢˜")
    ],
    outputs=[
        gr.Textbox(label="ç­”æ¡ˆ"),
        gr.Number(label="ç½®ä¿¡åº¦")
    ],
    title="SciResearcher - ç§‘ç ”æ–‡çŒ®åˆ†æåŠ©æ‰‹"
)

demo.launch()
```

### ğŸ“š å­¦ä¹ èµ„æºæ¨è

#### è¿›é˜¶é˜…è¯»

1. **Multi-Agentç³»ç»Ÿ**
   - Paper: "AutoGen: Enabling Next-Gen LLM Applications"
   - Book: "Multi-Agent Systems" by Gerhard Weiss

2. **RAGæŠ€æœ¯**
   - Paper: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
   - Tutorial: LangChain RAGå®˜æ–¹æ•™ç¨‹

3. **ç§‘ç ”åº”ç”¨**
   - Paper: "Scientific Discovery in the Age of AI"
   - Blog: Semantic ScholaræŠ€æœ¯åšå®¢

#### å¼€æºé¡¹ç›®å‚è€ƒ

```
ç›¸ä¼¼é¡¹ç›®:
â”œâ”€â”€ PaperQA - https://github.com/whitead/paper-qa
â”œâ”€â”€ LlamaIndex - https://github.com/run-llama/llama_index
â”œâ”€â”€ LangChain - https://github.com/langchain-ai/langchain
â””â”€â”€ AutoGen - https://github.com/microsoft/autogen

å·®å¼‚åŒ–:
âœ… SciResearcher: ä¸“æ³¨ç§‘ç ”ã€å¤šæ¨¡æ€ã€è½»é‡çº§
```

---

## ä¸‹ä¸€æ­¥

å­¦å®Œè¿™ä¸ªæ•™ç¨‹å’Œè¿›é˜¶å†…å®¹,ä½ åº”è¯¥èƒ½å¤Ÿ:

âœ… ç†è§£SciResearcherçš„æ•´ä½“æ¶æ„
âœ… ç‹¬ç«‹è¿è¡Œè®ºæ–‡åˆ†ææµç¨‹
âœ… ä¿®æ”¹å‚æ•°å’Œé…ç½®
âœ… è°ƒè¯•ç®€å•çš„é”™è¯¯
âœ… æ‰©å±•æ–°åŠŸèƒ½
âœ… ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

**ç»§ç»­å­¦ä¹ **:
- ğŸ“– é˜…è¯» smolagentså®˜æ–¹æ–‡æ¡£
- ğŸ“– é˜…è¯» Qwen3æ¨¡å‹æ–‡æ¡£
- ğŸ“– é˜…è¯» ChromaDBæ–‡æ¡£
- ğŸ”§ å®ç°ä¸€ä¸ªè‡ªå®šä¹‰Agent
- ğŸŒŸ å°è¯•ä¸€ä¸ªæ‰©å±•æ–¹å‘

**å‚è€ƒèµ„æ–™**:
- Smolagents: https://huggingface.co/docs/smolagents
- Qwen3: https://help.aliyun.com/zh/dashscope/
- ChromaDB: https://docs.trychroma.com/
- MinerU: https://mineru.net/docs

---

## é™„å½•: å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# ç¯å¢ƒå‡†å¤‡
python -m venv venv
source venv/bin/activate  # Mac/Linux
pip install -r requirements.txt

# é…ç½®
cp .env.example .env
nano .env

# è¿è¡Œæµ‹è¯•
python test_agent.py

# æŸ¥çœ‹å‘é‡æ•°
python -c "from tools.research_tools import vector_service; print(vector_service.collection.count())"

# æ¸…ç©ºå‘é‡åº“
python -c "from tools.research_tools import vector_service; vector_service.reset()"
```

---

**ğŸ‰ æ­å–œä½ å®Œæˆäº†åˆå­¦è€…æ•™ç¨‹!**

å¦‚æœæœ‰é—®é¢˜,æ¬¢è¿æŸ¥çœ‹:
- å®Œæ•´æŠ€æœ¯æ–‡æ¡£: `claudedocs/refactoring_summary.md`
- å‘é‡æ•°æ®åº“è¿ç§»: `claudedocs/vector_db_migration.md`
